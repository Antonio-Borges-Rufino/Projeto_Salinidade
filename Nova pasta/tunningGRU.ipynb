{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tunningGRU.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRF_-fOSVQE6"
      },
      "source": [
        "#1. Tunning de parametros para rede LSTM\n",
        "##2. especificações do tunning\n",
        "\n",
        "Camadas ocultas|Neuronios|Taxa de aprendizagem|Função de aprendizagem| Epocas\n",
        "---------------|---------|--------------------|---------|---------|\n",
        "        1      |  10-x+5-100   | 0.001 - 0.01       |Relu| 10-x+10- 100      \n",
        "  \n",
        "        \n",
        "##3. importar as bibliotecas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m261eM9_Vpri"
      },
      "source": [
        "###3.1 instalar a biblioteca do keras prórpia para o tunning\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1SsDpXj7NTh",
        "outputId": "3c92db4d-4eba-456d-d890-e51f2cafb1e4"
      },
      "source": [
        "pip install -q -U keras-tuner"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |███▍                            | 10 kB 23.9 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 20 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 30 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 40 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 51 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 61 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 71 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 81 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 92 kB 3.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 98 kB 2.7 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-S4bmiOOVuUA"
      },
      "source": [
        "###3.2 importar todas as bibliotecas que serão usadas no processo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2T-gFxa6165"
      },
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import keras_tuner as kt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDg3aUd0V0UF"
      },
      "source": [
        "##4. Importar os dataset e deixar o dataset na estrutura de entrada"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlhijEbf7MB4"
      },
      "source": [
        "df_teste = pd.read_parquet('/content/drive/MyDrive/df_teste_norm')\n",
        "df_treino = pd.read_parquet('/content/drive/MyDrive/df_treino_norm')\n",
        "df_treino = df_treino.dropna()\n",
        "df_teste = df_teste.dropna()\n",
        "df_treino2 = df_treino[['TEMP_NORM','BATIMETRIA_NORM']]\n",
        "df_teste2 = df_teste[['TEMP_NORM','BATIMETRIA_NORM']]\n",
        "lista = list()\n",
        "for x,i in df_treino2.values:\n",
        "  lista.append([x,i])\n",
        "sal_treino = np.array(df_treino.SALINIDADE)\n",
        "pr_treino = np.array(lista)\n",
        "lista = list()\n",
        "for x,i in df_teste2.values:\n",
        "  lista.append([x,i])\n",
        "sal_teste = np.array(df_teste.SALINIDADE)\n",
        "pr_teste = np.array(lista)\n",
        "pr_teste = pr_teste.reshape(pr_teste.shape[0],pr_teste.shape[1],1)\n",
        "pr_treino = pr_treino.reshape(pr_treino.shape[0],pr_treino.shape[1],1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoYbAtcCWTg0"
      },
      "source": [
        "## 5. Fazer o tunning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kuz4QZlH7UaD",
        "outputId": "0b741945-3460-48fe-b429-3e517ff64077"
      },
      "source": [
        "def KerasTunnerGRU(hp):\n",
        "  input = tf.keras.layers.Input(shape=(2,1), name='Bat_Temp')\n",
        "  hiper_param = hp.Int('units', min_value=10, max_value=100, step=5)\n",
        "  GRU = tf.keras.layers.GRU(units=hiper_param,activation='relu')(input)\n",
        "  output = tf.keras.layers.Dense(1,name='salinidade')(GRU)\n",
        "  modelo = tf.keras.Model(inputs=input, outputs=output)\n",
        "  taxa_apren =  hp.Choice('learning_rate', values=[0.001,0.002,0.003,0.004,0.005,0.006,0.007,0.008,0.009,0.01])\n",
        "  adan = tf.keras.optimizers.Adam(learning_rate=taxa_apren)\n",
        "  modelo.compile(optimizer=adan, loss='mse', metrics=['mae','mse'])\n",
        "  return modelo\n",
        "\n",
        "tuner = kt.Hyperband(KerasTunnerGRU,\n",
        "                     objective='val_mae',\n",
        "                     max_epochs=10,\n",
        "                     factor=3,\n",
        "                     directory='my_dir2',\n",
        "                     project_name='salinidade')\n",
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "tuner.search(pr_teste, sal_teste,epochs=10,batch_size=120,validation_split=0.2, callbacks=[stop_early])\n",
        "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 30 Complete [00h 03m 23s]\n",
            "val_mae: 0.867779552936554\n",
            "\n",
            "Best val_mae So Far: 0.867779552936554\n",
            "Total elapsed time: 00h 37m 30s\n",
            "INFO:tensorflow:Oracle triggered exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQye4PWXWYWl"
      },
      "source": [
        "###5.1 Resultado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IM-WYJEvIgfw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26fc1188-8a71-4b19-a4ee-4f64426c2030"
      },
      "source": [
        "best_hps.values"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'learning_rate': 0.001,\n",
              " 'tuner/bracket': 0,\n",
              " 'tuner/epochs': 10,\n",
              " 'tuner/initial_epoch': 0,\n",
              " 'tuner/round': 0,\n",
              " 'units': 85}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKUuXZXJWcjP"
      },
      "source": [
        "##6. Melhor época para os dados de teste"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dal2K1eUWyIF"
      },
      "source": [
        "###6.1 criar função"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8VHrhoyW08P"
      },
      "source": [
        "def KerasTunnerGRU():\n",
        "  input = tf.keras.layers.Input(shape=(2,1), name='Bat_Temp')\n",
        "  GRU = tf.keras.layers.GRU(units=best_hps['units'],activation='relu')(input)\n",
        "  output = tf.keras.layers.Dense(1,name='salinidade')(GRU)\n",
        "  modelo = tf.keras.Model(inputs=input, outputs=output)\n",
        "  adan = tf.keras.optimizers.Adam(learning_rate=best_hps['learning_rate'])\n",
        "  modelo.compile(optimizer=adan, loss='mse', metrics=['mae','mse'])\n",
        "  return modelo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOnR6Y2dXBiy"
      },
      "source": [
        "###6.2 treinar o modelo com os dados de teste para a melhor epoca"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDv1cLHAWh1-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cce215cf-497a-43e8-ad24-1de477063801"
      },
      "source": [
        "modelo  = KerasTunnerGRU()\n",
        "best_epoch = modelo.fit(pr_treino,sal_treino,epochs=100,batch_size=120)\n",
        "val_acc_per_epoch = best_epoch.history['mae']\n",
        "best_epoch = val_acc_per_epoch.index(min(val_acc_per_epoch))+1\n",
        "modelo.fit(pr_treino,sal_treino,epochs=best_epoch,batch_size=120)\n",
        "modelo.evaluate(pr_teste,sal_teste)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "10093/10093 [==============================] - 47s 5ms/step - loss: 16.0800 - mae: 1.3300 - mse: 16.0800\n",
            "Epoch 2/100\n",
            "10093/10093 [==============================] - 47s 5ms/step - loss: 4.5029 - mae: 0.9424 - mse: 4.5029\n",
            "Epoch 3/100\n",
            "10093/10093 [==============================] - 47s 5ms/step - loss: 4.4539 - mae: 0.9336 - mse: 4.4539\n",
            "Epoch 4/100\n",
            "10093/10093 [==============================] - 46s 5ms/step - loss: 4.4119 - mae: 0.9229 - mse: 4.4119\n",
            "Epoch 5/100\n",
            "10093/10093 [==============================] - 46s 5ms/step - loss: 4.3934 - mae: 0.9174 - mse: 4.3934\n",
            "Epoch 6/100\n",
            "10093/10093 [==============================] - 46s 5ms/step - loss: 4.3854 - mae: 0.9147 - mse: 4.3854\n",
            "Epoch 7/100\n",
            "10093/10093 [==============================] - 47s 5ms/step - loss: 4.3697 - mae: 0.9106 - mse: 4.3697\n",
            "Epoch 8/100\n",
            "10093/10093 [==============================] - 48s 5ms/step - loss: 4.3668 - mae: 0.9096 - mse: 4.3668\n",
            "Epoch 9/100\n",
            "10093/10093 [==============================] - 48s 5ms/step - loss: 4.3549 - mae: 0.9058 - mse: 4.3549\n",
            "Epoch 10/100\n",
            "10093/10093 [==============================] - 47s 5ms/step - loss: 4.3510 - mae: 0.9041 - mse: 4.3510\n",
            "Epoch 11/100\n",
            "10093/10093 [==============================] - 47s 5ms/step - loss: 4.3513 - mae: 0.9039 - mse: 4.3513\n",
            "Epoch 12/100\n",
            "10093/10093 [==============================] - 46s 5ms/step - loss: 4.3492 - mae: 0.9047 - mse: 4.3492\n",
            "Epoch 13/100\n",
            "10093/10093 [==============================] - 47s 5ms/step - loss: 4.3549 - mae: 0.9054 - mse: 4.3549\n",
            "Epoch 14/100\n",
            "10093/10093 [==============================] - 48s 5ms/step - loss: 4.3472 - mae: 0.9029 - mse: 4.3472\n",
            "Epoch 15/100\n",
            "10093/10093 [==============================] - 47s 5ms/step - loss: 4.3533 - mae: 0.9053 - mse: 4.3533\n",
            "Epoch 16/100\n",
            "10093/10093 [==============================] - 45s 4ms/step - loss: 4.3424 - mae: 0.9018 - mse: 4.3424\n",
            "Epoch 17/100\n",
            "10093/10093 [==============================] - 46s 5ms/step - loss: 4.3747 - mae: 0.9122 - mse: 4.3747\n",
            "Epoch 18/100\n",
            "10093/10093 [==============================] - 47s 5ms/step - loss: 4.3747 - mae: 0.9127 - mse: 4.3747\n",
            "Epoch 19/100\n",
            "10093/10093 [==============================] - 47s 5ms/step - loss: 4.3473 - mae: 0.9020 - mse: 4.3473\n",
            "Epoch 20/100\n",
            "10093/10093 [==============================] - 46s 5ms/step - loss: 4.3412 - mae: 0.9003 - mse: 4.3412\n",
            "Epoch 21/100\n",
            "10093/10093 [==============================] - 47s 5ms/step - loss: 4.3240 - mae: 0.8930 - mse: 4.3240\n",
            "Epoch 22/100\n",
            "10093/10093 [==============================] - 46s 5ms/step - loss: 4.3215 - mae: 0.8926 - mse: 4.3215\n",
            "Epoch 23/100\n",
            "10093/10093 [==============================] - 48s 5ms/step - loss: 4.3170 - mae: 0.8922 - mse: 4.3170\n",
            "Epoch 24/100\n",
            "10093/10093 [==============================] - 47s 5ms/step - loss: 4.3089 - mae: 0.8888 - mse: 4.3089\n",
            "Epoch 25/100\n",
            "10093/10093 [==============================] - 46s 5ms/step - loss: 4.3052 - mae: 0.8882 - mse: 4.3052\n",
            "Epoch 26/100\n",
            "10093/10093 [==============================] - 46s 5ms/step - loss: 4.3022 - mae: 0.8867 - mse: 4.3022\n",
            "Epoch 27/100\n",
            "10093/10093 [==============================] - 46s 5ms/step - loss: 4.2968 - mae: 0.8854 - mse: 4.2968\n",
            "Epoch 28/100\n",
            "10093/10093 [==============================] - 49s 5ms/step - loss: 4.2981 - mae: 0.8863 - mse: 4.2981\n",
            "Epoch 29/100\n",
            "10093/10093 [==============================] - 50s 5ms/step - loss: 4.2951 - mae: 0.8849 - mse: 4.2951\n",
            "Epoch 30/100\n",
            "10093/10093 [==============================] - 49s 5ms/step - loss: 4.2920 - mae: 0.8848 - mse: 4.2920\n",
            "Epoch 31/100\n",
            "10093/10093 [==============================] - 48s 5ms/step - loss: 4.2890 - mae: 0.8841 - mse: 4.2890\n",
            "Epoch 32/100\n",
            "10093/10093 [==============================] - 47s 5ms/step - loss: 4.2872 - mae: 0.8840 - mse: 4.2872\n",
            "Epoch 33/100\n",
            "10093/10093 [==============================] - 47s 5ms/step - loss: 4.2838 - mae: 0.8832 - mse: 4.2838\n",
            "Epoch 34/100\n",
            "10093/10093 [==============================] - 47s 5ms/step - loss: 4.2847 - mae: 0.8829 - mse: 4.2847\n",
            "Epoch 35/100\n",
            "10093/10093 [==============================] - 46s 5ms/step - loss: 4.2828 - mae: 0.8835 - mse: 4.2828\n",
            "Epoch 36/100\n",
            "10093/10093 [==============================] - 46s 5ms/step - loss: 4.2786 - mae: 0.8808 - mse: 4.2786\n",
            "Epoch 37/100\n",
            "10093/10093 [==============================] - 47s 5ms/step - loss: 4.2782 - mae: 0.8812 - mse: 4.2782\n",
            "Epoch 38/100\n",
            "10093/10093 [==============================] - 48s 5ms/step - loss: 4.2739 - mae: 0.8799 - mse: 4.2739\n",
            "Epoch 39/100\n",
            "10093/10093 [==============================] - 49s 5ms/step - loss: 4.2731 - mae: 0.8797 - mse: 4.2731\n",
            "Epoch 40/100\n",
            "10093/10093 [==============================] - 49s 5ms/step - loss: 4.2732 - mae: 0.8811 - mse: 4.2732\n",
            "Epoch 41/100\n",
            "10093/10093 [==============================] - 48s 5ms/step - loss: 4.2716 - mae: 0.8808 - mse: 4.2716\n",
            "Epoch 42/100\n",
            "10093/10093 [==============================] - 47s 5ms/step - loss: 4.2676 - mae: 0.8797 - mse: 4.2676\n",
            "Epoch 43/100\n",
            "10093/10093 [==============================] - 48s 5ms/step - loss: 4.2658 - mae: 0.8800 - mse: 4.2658\n",
            "Epoch 44/100\n",
            "10093/10093 [==============================] - 49s 5ms/step - loss: 4.2614 - mae: 0.8787 - mse: 4.2614\n",
            "Epoch 45/100\n",
            "10093/10093 [==============================] - 49s 5ms/step - loss: 4.2654 - mae: 0.8809 - mse: 4.2654\n",
            "Epoch 46/100\n",
            "10093/10093 [==============================] - 49s 5ms/step - loss: 4.2613 - mae: 0.8788 - mse: 4.2613\n",
            "Epoch 47/100\n",
            "10093/10093 [==============================] - 50s 5ms/step - loss: 4.2619 - mae: 0.8795 - mse: 4.2619\n",
            "Epoch 48/100\n",
            "10093/10093 [==============================] - 48s 5ms/step - loss: 4.2610 - mae: 0.8794 - mse: 4.2610\n",
            "Epoch 49/100\n",
            "10093/10093 [==============================] - 48s 5ms/step - loss: 4.2588 - mae: 0.8786 - mse: 4.2588\n",
            "Epoch 50/100\n",
            "10093/10093 [==============================] - 47s 5ms/step - loss: 4.2614 - mae: 0.8799 - mse: 4.2614\n",
            "Epoch 51/100\n",
            "10093/10093 [==============================] - 46s 5ms/step - loss: 4.2584 - mae: 0.8797 - mse: 4.2584\n",
            "Epoch 52/100\n",
            "10093/10093 [==============================] - 46s 5ms/step - loss: 4.2546 - mae: 0.8782 - mse: 4.2546\n",
            "Epoch 53/100\n",
            "10093/10093 [==============================] - 47s 5ms/step - loss: 4.2546 - mae: 0.8789 - mse: 4.2546\n",
            "Epoch 54/100\n",
            "10093/10093 [==============================] - 47s 5ms/step - loss: 4.2520 - mae: 0.8780 - mse: 4.2520\n",
            "Epoch 55/100\n",
            "10093/10093 [==============================] - 47s 5ms/step - loss: 4.2503 - mae: 0.8786 - mse: 4.2503\n",
            "Epoch 56/100\n",
            "10093/10093 [==============================] - 46s 5ms/step - loss: 4.2528 - mae: 0.8788 - mse: 4.2528\n",
            "Epoch 57/100\n",
            "10093/10093 [==============================] - 46s 5ms/step - loss: 4.2475 - mae: 0.8780 - mse: 4.2475\n",
            "Epoch 58/100\n",
            "10093/10093 [==============================] - 47s 5ms/step - loss: 4.2464 - mae: 0.8771 - mse: 4.2464\n",
            "Epoch 59/100\n",
            "10093/10093 [==============================] - 48s 5ms/step - loss: 4.2413 - mae: 0.8755 - mse: 4.2413\n",
            "Epoch 60/100\n",
            "10093/10093 [==============================] - 48s 5ms/step - loss: 4.2405 - mae: 0.8743 - mse: 4.2405\n",
            "Epoch 61/100\n",
            "10093/10093 [==============================] - 48s 5ms/step - loss: 4.2363 - mae: 0.8736 - mse: 4.2363\n",
            "Epoch 62/100\n",
            "10093/10093 [==============================] - 48s 5ms/step - loss: 4.2337 - mae: 0.8717 - mse: 4.2337\n",
            "Epoch 63/100\n",
            "10093/10093 [==============================] - 49s 5ms/step - loss: 4.2316 - mae: 0.8710 - mse: 4.2316\n",
            "Epoch 64/100\n",
            "10093/10093 [==============================] - 49s 5ms/step - loss: 4.2279 - mae: 0.8696 - mse: 4.2279\n",
            "Epoch 65/100\n",
            "10093/10093 [==============================] - 48s 5ms/step - loss: 4.2263 - mae: 0.8687 - mse: 4.2263\n",
            "Epoch 66/100\n",
            "10093/10093 [==============================] - 49s 5ms/step - loss: 4.2237 - mae: 0.8685 - mse: 4.2237\n",
            "Epoch 67/100\n",
            "10093/10093 [==============================] - 50s 5ms/step - loss: 4.2216 - mae: 0.8675 - mse: 4.2216\n",
            "Epoch 68/100\n",
            "10093/10093 [==============================] - 48s 5ms/step - loss: 4.2199 - mae: 0.8669 - mse: 4.2199\n",
            "Epoch 69/100\n",
            "10093/10093 [==============================] - 48s 5ms/step - loss: 4.2181 - mae: 0.8667 - mse: 4.2181\n",
            "Epoch 70/100\n",
            "10093/10093 [==============================] - 47s 5ms/step - loss: 4.2150 - mae: 0.8658 - mse: 4.2150\n",
            "Epoch 71/100\n",
            "10093/10093 [==============================] - 47s 5ms/step - loss: 4.2126 - mae: 0.8652 - mse: 4.2126\n",
            "Epoch 72/100\n",
            "10093/10093 [==============================] - 47s 5ms/step - loss: 4.2116 - mae: 0.8642 - mse: 4.2116\n",
            "Epoch 73/100\n",
            "10093/10093 [==============================] - 48s 5ms/step - loss: 4.2102 - mae: 0.8646 - mse: 4.2102\n",
            "Epoch 74/100\n",
            "10093/10093 [==============================] - 49s 5ms/step - loss: 4.2109 - mae: 0.8643 - mse: 4.2109\n",
            "Epoch 75/100\n",
            "10093/10093 [==============================] - 49s 5ms/step - loss: 4.2091 - mae: 0.8637 - mse: 4.2091\n",
            "Epoch 76/100\n",
            "10093/10093 [==============================] - 47s 5ms/step - loss: 4.2083 - mae: 0.8636 - mse: 4.2083\n",
            "Epoch 77/100\n",
            "10093/10093 [==============================] - 47s 5ms/step - loss: 4.2072 - mae: 0.8635 - mse: 4.2072\n",
            "Epoch 78/100\n",
            "10093/10093 [==============================] - 48s 5ms/step - loss: 4.2065 - mae: 0.8632 - mse: 4.2065\n",
            "Epoch 79/100\n",
            "10093/10093 [==============================] - 48s 5ms/step - loss: 4.2064 - mae: 0.8632 - mse: 4.2064\n",
            "Epoch 80/100\n",
            "10093/10093 [==============================] - 48s 5ms/step - loss: 4.2055 - mae: 0.8629 - mse: 4.2055\n",
            "Epoch 81/100\n",
            "10093/10093 [==============================] - 48s 5ms/step - loss: 4.2051 - mae: 0.8624 - mse: 4.2051\n",
            "Epoch 82/100\n",
            "10093/10093 [==============================] - 48s 5ms/step - loss: 4.2050 - mae: 0.8629 - mse: 4.2050\n",
            "Epoch 83/100\n",
            "10093/10093 [==============================] - 49s 5ms/step - loss: 4.2020 - mae: 0.8620 - mse: 4.2020\n",
            "Epoch 84/100\n",
            "10093/10093 [==============================] - 49s 5ms/step - loss: 4.2027 - mae: 0.8621 - mse: 4.2027\n",
            "Epoch 85/100\n",
            "10093/10093 [==============================] - 49s 5ms/step - loss: 4.2024 - mae: 0.8624 - mse: 4.2024\n",
            "Epoch 86/100\n",
            "10093/10093 [==============================] - 49s 5ms/step - loss: 4.2000 - mae: 0.8612 - mse: 4.2000\n",
            "Epoch 87/100\n",
            "10093/10093 [==============================] - 49s 5ms/step - loss: 4.2011 - mae: 0.8621 - mse: 4.2011\n",
            "Epoch 88/100\n",
            "10093/10093 [==============================] - 50s 5ms/step - loss: 4.2005 - mae: 0.8623 - mse: 4.2005\n",
            "Epoch 89/100\n",
            "10093/10093 [==============================] - 48s 5ms/step - loss: 4.1985 - mae: 0.8614 - mse: 4.1985\n",
            "Epoch 90/100\n",
            "10093/10093 [==============================] - 47s 5ms/step - loss: 4.1990 - mae: 0.8609 - mse: 4.1990\n",
            "Epoch 91/100\n",
            "10093/10093 [==============================] - 48s 5ms/step - loss: 4.1998 - mae: 0.8619 - mse: 4.1998\n",
            "Epoch 92/100\n",
            "10093/10093 [==============================] - 48s 5ms/step - loss: 4.1998 - mae: 0.8617 - mse: 4.1998\n",
            "Epoch 93/100\n",
            "10093/10093 [==============================] - 49s 5ms/step - loss: 4.1973 - mae: 0.8604 - mse: 4.1973\n",
            "Epoch 94/100\n",
            "10093/10093 [==============================] - 49s 5ms/step - loss: 4.1990 - mae: 0.8610 - mse: 4.1990\n",
            "Epoch 95/100\n",
            "10093/10093 [==============================] - 49s 5ms/step - loss: 4.1968 - mae: 0.8610 - mse: 4.1968\n",
            "Epoch 96/100\n",
            "10093/10093 [==============================] - 48s 5ms/step - loss: 4.1967 - mae: 0.8603 - mse: 4.1967\n",
            "Epoch 97/100\n",
            "10093/10093 [==============================] - 47s 5ms/step - loss: 4.1959 - mae: 0.8603 - mse: 4.1959\n",
            "Epoch 98/100\n",
            "10093/10093 [==============================] - 48s 5ms/step - loss: 4.1964 - mae: 0.8603 - mse: 4.1964\n",
            "Epoch 99/100\n",
            "10093/10093 [==============================] - 48s 5ms/step - loss: 4.1939 - mae: 0.8598 - mse: 4.1939\n",
            "Epoch 100/100\n",
            "10093/10093 [==============================] - 47s 5ms/step - loss: 4.1948 - mae: 0.8598 - mse: 4.1948\n",
            "Epoch 1/99\n",
            "10093/10093 [==============================] - 47s 5ms/step - loss: 4.1929 - mae: 0.8594 - mse: 4.1929\n",
            "Epoch 2/99\n",
            "10093/10093 [==============================] - 48s 5ms/step - loss: 4.1923 - mae: 0.8590 - mse: 4.1923\n",
            "Epoch 3/99\n",
            "10093/10093 [==============================] - 49s 5ms/step - loss: 4.1908 - mae: 0.8586 - mse: 4.1908\n",
            "Epoch 4/99\n",
            "10093/10093 [==============================] - 48s 5ms/step - loss: 4.1915 - mae: 0.8581 - mse: 4.1915\n",
            "Epoch 5/99\n",
            "10093/10093 [==============================] - 47s 5ms/step - loss: 4.1917 - mae: 0.8584 - mse: 4.1917\n",
            "Epoch 6/99\n",
            "10093/10093 [==============================] - 48s 5ms/step - loss: 4.1906 - mae: 0.8572 - mse: 4.1906\n",
            "Epoch 7/99\n",
            "10093/10093 [==============================] - 47s 5ms/step - loss: 4.1907 - mae: 0.8578 - mse: 4.1907\n",
            "Epoch 8/99\n",
            "10093/10093 [==============================] - 48s 5ms/step - loss: 4.1888 - mae: 0.8569 - mse: 4.1888\n",
            "Epoch 9/99\n",
            "10093/10093 [==============================] - 49s 5ms/step - loss: 4.1895 - mae: 0.8576 - mse: 4.1895\n",
            "Epoch 10/99\n",
            "10093/10093 [==============================] - 50s 5ms/step - loss: 4.1887 - mae: 0.8572 - mse: 4.1887\n",
            "Epoch 11/99\n",
            "10093/10093 [==============================] - 50s 5ms/step - loss: 4.1889 - mae: 0.8573 - mse: 4.1889\n",
            "Epoch 12/99\n",
            "10093/10093 [==============================] - 51s 5ms/step - loss: 4.1876 - mae: 0.8569 - mse: 4.1876\n",
            "Epoch 13/99\n",
            "10093/10093 [==============================] - 53s 5ms/step - loss: 4.1870 - mae: 0.8570 - mse: 4.1870\n",
            "Epoch 14/99\n",
            "10093/10093 [==============================] - 52s 5ms/step - loss: 4.1884 - mae: 0.8572 - mse: 4.1884\n",
            "Epoch 15/99\n",
            "10093/10093 [==============================] - 52s 5ms/step - loss: 4.1871 - mae: 0.8567 - mse: 4.1871\n",
            "Epoch 16/99\n",
            "10093/10093 [==============================] - 53s 5ms/step - loss: 4.1870 - mae: 0.8562 - mse: 4.1870\n",
            "Epoch 17/99\n",
            "10093/10093 [==============================] - 52s 5ms/step - loss: 4.1880 - mae: 0.8572 - mse: 4.1880\n",
            "Epoch 18/99\n",
            "10093/10093 [==============================] - 52s 5ms/step - loss: 4.1879 - mae: 0.8574 - mse: 4.1879\n",
            "Epoch 19/99\n",
            "10093/10093 [==============================] - 52s 5ms/step - loss: 4.1872 - mae: 0.8574 - mse: 4.1872\n",
            "Epoch 20/99\n",
            "10093/10093 [==============================] - 51s 5ms/step - loss: 4.1875 - mae: 0.8573 - mse: 4.1875\n",
            "Epoch 21/99\n",
            "10093/10093 [==============================] - 50s 5ms/step - loss: 4.1861 - mae: 0.8566 - mse: 4.1861\n",
            "Epoch 22/99\n",
            "10093/10093 [==============================] - 49s 5ms/step - loss: 4.1856 - mae: 0.8566 - mse: 4.1856\n",
            "Epoch 23/99\n",
            "10093/10093 [==============================] - 49s 5ms/step - loss: 4.1855 - mae: 0.8568 - mse: 4.1855\n",
            "Epoch 24/99\n",
            "10093/10093 [==============================] - 49s 5ms/step - loss: 4.1853 - mae: 0.8563 - mse: 4.1853\n",
            "Epoch 25/99\n",
            "10093/10093 [==============================] - 48s 5ms/step - loss: 4.1853 - mae: 0.8568 - mse: 4.1853\n",
            "Epoch 26/99\n",
            "10093/10093 [==============================] - 49s 5ms/step - loss: 4.1858 - mae: 0.8569 - mse: 4.1858\n",
            "Epoch 27/99\n",
            "10093/10093 [==============================] - 49s 5ms/step - loss: 4.1850 - mae: 0.8566 - mse: 4.1850\n",
            "Epoch 28/99\n",
            "10093/10093 [==============================] - 49s 5ms/step - loss: 4.1841 - mae: 0.8563 - mse: 4.1841\n",
            "Epoch 29/99\n",
            "10093/10093 [==============================] - 50s 5ms/step - loss: 4.1854 - mae: 0.8567 - mse: 4.1854\n",
            "Epoch 30/99\n",
            "10093/10093 [==============================] - 51s 5ms/step - loss: 4.1855 - mae: 0.8567 - mse: 4.1855\n",
            "Epoch 31/99\n",
            "10093/10093 [==============================] - 50s 5ms/step - loss: 4.1841 - mae: 0.8563 - mse: 4.1841\n",
            "Epoch 32/99\n",
            "10093/10093 [==============================] - 50s 5ms/step - loss: 4.1849 - mae: 0.8569 - mse: 4.1849\n",
            "Epoch 33/99\n",
            "10093/10093 [==============================] - 51s 5ms/step - loss: 4.1849 - mae: 0.8569 - mse: 4.1849\n",
            "Epoch 34/99\n",
            "10093/10093 [==============================] - 51s 5ms/step - loss: 4.1827 - mae: 0.8559 - mse: 4.1827\n",
            "Epoch 35/99\n",
            "10093/10093 [==============================] - 51s 5ms/step - loss: 4.1843 - mae: 0.8563 - mse: 4.1843\n",
            "Epoch 36/99\n",
            "10093/10093 [==============================] - 51s 5ms/step - loss: 4.1820 - mae: 0.8559 - mse: 4.1820\n",
            "Epoch 37/99\n",
            "10093/10093 [==============================] - 52s 5ms/step - loss: 4.1831 - mae: 0.8561 - mse: 4.1831\n",
            "Epoch 38/99\n",
            "10093/10093 [==============================] - 51s 5ms/step - loss: 4.1837 - mae: 0.8564 - mse: 4.1837\n",
            "Epoch 39/99\n",
            "10093/10093 [==============================] - 51s 5ms/step - loss: 4.1842 - mae: 0.8564 - mse: 4.1842\n",
            "Epoch 40/99\n",
            "10093/10093 [==============================] - 50s 5ms/step - loss: 4.1843 - mae: 0.8565 - mse: 4.1843\n",
            "Epoch 41/99\n",
            "10093/10093 [==============================] - 50s 5ms/step - loss: 4.1834 - mae: 0.8562 - mse: 4.1834\n",
            "Epoch 42/99\n",
            "10093/10093 [==============================] - 51s 5ms/step - loss: 4.1831 - mae: 0.8559 - mse: 4.1831\n",
            "Epoch 43/99\n",
            "10093/10093 [==============================] - 51s 5ms/step - loss: 4.1836 - mae: 0.8561 - mse: 4.1836\n",
            "Epoch 44/99\n",
            "10093/10093 [==============================] - 50s 5ms/step - loss: 4.1838 - mae: 0.8566 - mse: 4.1838\n",
            "Epoch 45/99\n",
            "10093/10093 [==============================] - 50s 5ms/step - loss: 4.1818 - mae: 0.8553 - mse: 4.1818\n",
            "Epoch 46/99\n",
            "10093/10093 [==============================] - 51s 5ms/step - loss: 4.1824 - mae: 0.8557 - mse: 4.1824\n",
            "Epoch 47/99\n",
            "10093/10093 [==============================] - 51s 5ms/step - loss: 4.1820 - mae: 0.8555 - mse: 4.1820\n",
            "Epoch 48/99\n",
            "10093/10093 [==============================] - 49s 5ms/step - loss: 4.1815 - mae: 0.8551 - mse: 4.1815\n",
            "Epoch 49/99\n",
            "10093/10093 [==============================] - 49s 5ms/step - loss: 4.1807 - mae: 0.8552 - mse: 4.1807\n",
            "Epoch 50/99\n",
            "10093/10093 [==============================] - 50s 5ms/step - loss: 4.1821 - mae: 0.8554 - mse: 4.1821\n",
            "Epoch 51/99\n",
            "10093/10093 [==============================] - 50s 5ms/step - loss: 4.1808 - mae: 0.8555 - mse: 4.1808\n",
            "Epoch 52/99\n",
            "10093/10093 [==============================] - 49s 5ms/step - loss: 4.1814 - mae: 0.8557 - mse: 4.1814\n",
            "Epoch 53/99\n",
            "10093/10093 [==============================] - 50s 5ms/step - loss: 4.1805 - mae: 0.8555 - mse: 4.1805\n",
            "Epoch 54/99\n",
            "10093/10093 [==============================] - 50s 5ms/step - loss: 4.1816 - mae: 0.8558 - mse: 4.1816\n",
            "Epoch 55/99\n",
            "10093/10093 [==============================] - 50s 5ms/step - loss: 4.1792 - mae: 0.8548 - mse: 4.1792\n",
            "Epoch 56/99\n",
            "10093/10093 [==============================] - 51s 5ms/step - loss: 4.1804 - mae: 0.8551 - mse: 4.1804\n",
            "Epoch 57/99\n",
            "10093/10093 [==============================] - 49s 5ms/step - loss: 4.1800 - mae: 0.8550 - mse: 4.1800\n",
            "Epoch 58/99\n",
            "10093/10093 [==============================] - 50s 5ms/step - loss: 4.1808 - mae: 0.8557 - mse: 4.1808\n",
            "Epoch 59/99\n",
            "10093/10093 [==============================] - 50s 5ms/step - loss: 4.1792 - mae: 0.8553 - mse: 4.1792\n",
            "Epoch 60/99\n",
            "10093/10093 [==============================] - 49s 5ms/step - loss: 4.1799 - mae: 0.8552 - mse: 4.1799\n",
            "Epoch 61/99\n",
            "10093/10093 [==============================] - 51s 5ms/step - loss: 4.1801 - mae: 0.8548 - mse: 4.1801\n",
            "Epoch 62/99\n",
            "10093/10093 [==============================] - 51s 5ms/step - loss: 4.1791 - mae: 0.8550 - mse: 4.1791\n",
            "Epoch 63/99\n",
            "10093/10093 [==============================] - 49s 5ms/step - loss: 4.1807 - mae: 0.8555 - mse: 4.1807\n",
            "Epoch 64/99\n",
            "10093/10093 [==============================] - 49s 5ms/step - loss: 4.1795 - mae: 0.8556 - mse: 4.1795\n",
            "Epoch 65/99\n",
            "10093/10093 [==============================] - 48s 5ms/step - loss: 4.1796 - mae: 0.8550 - mse: 4.1796\n",
            "Epoch 66/99\n",
            "10093/10093 [==============================] - 51s 5ms/step - loss: 4.1795 - mae: 0.8556 - mse: 4.1795\n",
            "Epoch 67/99\n",
            "10093/10093 [==============================] - 51s 5ms/step - loss: 4.1790 - mae: 0.8555 - mse: 4.1790\n",
            "Epoch 68/99\n",
            "10093/10093 [==============================] - 52s 5ms/step - loss: 4.1780 - mae: 0.8545 - mse: 4.1780\n",
            "Epoch 69/99\n",
            "10093/10093 [==============================] - 51s 5ms/step - loss: 4.1778 - mae: 0.8543 - mse: 4.1778\n",
            "Epoch 70/99\n",
            "10093/10093 [==============================] - 50s 5ms/step - loss: 4.1783 - mae: 0.8545 - mse: 4.1783\n",
            "Epoch 71/99\n",
            "10093/10093 [==============================] - 50s 5ms/step - loss: 4.1803 - mae: 0.8557 - mse: 4.1803\n",
            "Epoch 72/99\n",
            "10093/10093 [==============================] - 49s 5ms/step - loss: 4.1793 - mae: 0.8551 - mse: 4.1793\n",
            "Epoch 73/99\n",
            "10093/10093 [==============================] - 49s 5ms/step - loss: 4.1791 - mae: 0.8547 - mse: 4.1791\n",
            "Epoch 74/99\n",
            "10093/10093 [==============================] - 50s 5ms/step - loss: 4.1781 - mae: 0.8553 - mse: 4.1781\n",
            "Epoch 75/99\n",
            "10093/10093 [==============================] - 51s 5ms/step - loss: 4.1771 - mae: 0.8548 - mse: 4.1771\n",
            "Epoch 76/99\n",
            "10093/10093 [==============================] - 51s 5ms/step - loss: 4.1783 - mae: 0.8551 - mse: 4.1783\n",
            "Epoch 77/99\n",
            "10093/10093 [==============================] - 49s 5ms/step - loss: 4.1787 - mae: 0.8549 - mse: 4.1787\n",
            "Epoch 78/99\n",
            "10093/10093 [==============================] - 49s 5ms/step - loss: 4.1784 - mae: 0.8545 - mse: 4.1784\n",
            "Epoch 79/99\n",
            "10093/10093 [==============================] - 49s 5ms/step - loss: 4.1779 - mae: 0.8541 - mse: 4.1779\n",
            "Epoch 80/99\n",
            "10093/10093 [==============================] - 51s 5ms/step - loss: 4.1789 - mae: 0.8553 - mse: 4.1789\n",
            "Epoch 81/99\n",
            "10093/10093 [==============================] - 51s 5ms/step - loss: 4.1785 - mae: 0.8546 - mse: 4.1785\n",
            "Epoch 82/99\n",
            "10093/10093 [==============================] - 49s 5ms/step - loss: 4.1780 - mae: 0.8543 - mse: 4.1780\n",
            "Epoch 83/99\n",
            "10093/10093 [==============================] - 50s 5ms/step - loss: 4.1777 - mae: 0.8547 - mse: 4.1777\n",
            "Epoch 84/99\n",
            "10093/10093 [==============================] - 49s 5ms/step - loss: 4.1757 - mae: 0.8541 - mse: 4.1757\n",
            "Epoch 85/99\n",
            "10093/10093 [==============================] - 50s 5ms/step - loss: 4.1764 - mae: 0.8544 - mse: 4.1764\n",
            "Epoch 86/99\n",
            "10093/10093 [==============================] - 50s 5ms/step - loss: 4.1761 - mae: 0.8542 - mse: 4.1761\n",
            "Epoch 87/99\n",
            "10093/10093 [==============================] - 50s 5ms/step - loss: 4.1789 - mae: 0.8549 - mse: 4.1789\n",
            "Epoch 88/99\n",
            "10093/10093 [==============================] - 49s 5ms/step - loss: 4.1763 - mae: 0.8545 - mse: 4.1763\n",
            "Epoch 89/99\n",
            "10093/10093 [==============================] - 49s 5ms/step - loss: 4.1759 - mae: 0.8542 - mse: 4.1759\n",
            "Epoch 90/99\n",
            "10093/10093 [==============================] - 49s 5ms/step - loss: 4.1772 - mae: 0.8545 - mse: 4.1772\n",
            "Epoch 91/99\n",
            "10093/10093 [==============================] - 49s 5ms/step - loss: 4.1768 - mae: 0.8540 - mse: 4.1768\n",
            "Epoch 92/99\n",
            "10093/10093 [==============================] - 49s 5ms/step - loss: 4.1760 - mae: 0.8541 - mse: 4.1760\n",
            "Epoch 93/99\n",
            "10093/10093 [==============================] - 49s 5ms/step - loss: 4.1761 - mae: 0.8547 - mse: 4.1761\n",
            "Epoch 94/99\n",
            "10093/10093 [==============================] - 50s 5ms/step - loss: 4.1763 - mae: 0.8544 - mse: 4.1763\n",
            "Epoch 95/99\n",
            "10093/10093 [==============================] - 50s 5ms/step - loss: 4.1754 - mae: 0.8542 - mse: 4.1754\n",
            "Epoch 96/99\n",
            "10093/10093 [==============================] - 49s 5ms/step - loss: 4.1762 - mae: 0.8539 - mse: 4.1762\n",
            "Epoch 97/99\n",
            "10093/10093 [==============================] - 48s 5ms/step - loss: 4.1755 - mae: 0.8538 - mse: 4.1755\n",
            "Epoch 98/99\n",
            "10093/10093 [==============================] - 48s 5ms/step - loss: 4.1760 - mae: 0.8539 - mse: 4.1760\n",
            "Epoch 99/99\n",
            "10093/10093 [==============================] - 49s 5ms/step - loss: 4.1757 - mae: 0.8544 - mse: 4.1757\n",
            "16221/16221 [==============================] - 30s 2ms/step - loss: 4.3002 - mae: 0.8641 - mse: 4.3002\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4.300154685974121, 0.864061713218689, 4.300154685974121]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvJqpRaj1c2K"
      },
      "source": [
        "# Resultado\n",
        "MAE: 0.864061713218689 \n",
        "\n",
        "MSE: 4.300154685974121\n",
        "\n",
        "TAXA DE APRENDIZAGEM: 0.001\n",
        "\n",
        "EPOCH: 99\n",
        "\n",
        "NEURONIOS: 85"
      ]
    }
  ]
}