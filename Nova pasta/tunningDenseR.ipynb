{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tunningDenseR.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Z4ayL0mU2_3u",
        "oL5y4-mk_HJU",
        "Iw2g0iQMBlMJ",
        "t6bLU9GQC3uN",
        "JozJp6frDrbi",
        "hiiqhjlLHEWH",
        "u-0TThdrIQei",
        "IzQB4MBNHE2X",
        "DgWkcuYuHFLG",
        "JV_9w7WYSLAm"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qpm336lM2Yfr"
      },
      "source": [
        "#1. Tunning de parametros para rede densa\n",
        "##2. especificações do tunning\n",
        "\n",
        "Camadas ocultas|Neuronios|Taxa de aprendizagem|Função de aprendizagem| Epocas\n",
        "---------------|---------|--------------------|---------|---------|\n",
        "        1      |  10-x+10-40   | 0.001 - 0.01       |Relu| 10-x+10- 100      \n",
        "        2      |  10-x+10-40    | 0.001 - 0.01       |Relu|10-x+10- 100\n",
        "        3      |  10-x+10-40    | 0.001 - 0.01       | Relu|10-x+10- 100\n",
        "        4      |  10-x+10-40    | 0.001 - 0.01       | Relu| 10-x+10- 100\n",
        "        5      |  10-x+10-40    | 0.001 - 0.01       | Relu | 10-x+10- 100\n",
        "        \n",
        "##3. importar as bibliotecas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIOPXI5-2re3"
      },
      "source": [
        "###3.1 instalar a biblioteca do keras prórpia para o tunning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBdSCR6Y2R47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21cde07b-5b24-4421-e44b-2c2e27d59c1b"
      },
      "source": [
        "pip install -q -U keras-tuner"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |███▍                            | 10 kB 22.6 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 20 kB 28.9 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 30 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 40 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 51 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 61 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 71 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 81 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 92 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 98 kB 3.1 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idbK138f20Na"
      },
      "source": [
        "###3.2 importar todas as bibliotecas que serão usadas no processo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNHAxP8d27Ut"
      },
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import keras_tuner as kt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4ayL0mU2_3u"
      },
      "source": [
        "##4. Importar os dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18DN-5aJ3JlY"
      },
      "source": [
        "df_teste = pd.read_parquet('/content/drive/MyDrive/df_teste_norm')\n",
        "df_treino = pd.read_parquet('/content/drive/MyDrive/df_treino_norm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qA452vrB3Kgh"
      },
      "source": [
        "###4.1 apesar de todo o pré-processamento já feito, alguns dados ainda ficaram nulos, para isso, vou tira-los. Tambem vou criar um df para os dados preditores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CivBxUZR3S97"
      },
      "source": [
        "df_treino = df_treino.dropna()\n",
        "df_teste = df_teste.dropna()\n",
        "df_treino2 = df_treino[['TEMP_NORM','BATIMETRIA_NORM']]\n",
        "df_teste2 = df_teste[['TEMP_NORM','BATIMETRIA_NORM']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9NJH_ud80-o"
      },
      "source": [
        "###4.2 criar os input e outputs da rede, tanto para treino quanto para teste"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-ImfG9D9SV4"
      },
      "source": [
        "####4.2.1 Treino:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cx6aOw1-9XZJ"
      },
      "source": [
        "lista = list()\n",
        "for x,i in df_treino2.values:\n",
        "  lista.append([x,i])\n",
        "sal_treino = np.array(df_treino.SALINIDADE)\n",
        "pr_treino = np.array(lista)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v24-yu2y9xYR"
      },
      "source": [
        "####4.1.2 Teste:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWWzEp5l92Kx"
      },
      "source": [
        "lista = list()\n",
        "for x,i in df_teste2.values:\n",
        "  lista.append([x,i])\n",
        "sal_teste = np.array(df_teste.SALINIDADE)\n",
        "pr_teste = np.array(lista)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oL5y4-mk_HJU"
      },
      "source": [
        "##5. Criar as funções de tunning utilizando a biblioteca keras tuner\n",
        "  > cada função tem como parametro hp, que é uma classe repassada na hora de fazer o tunning, dentro dela existem o hp.Int() que vai ser passado para o ajuste dos neuronios e hp.Choice() que vai ser passado para o ajuste das taxas de aprendizagem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rZXz6HdAYAL"
      },
      "source": [
        "###5.1 Função para 1 camada densa \n",
        "Neuronios|Taxa de aprendizagem|Função de aprendizagem\n",
        "---------------|--------------|----|\n",
        "        10     | 0.001 - 0.01 |Relu|    \n",
        "        20     | 0.001 - 0.01 |Relu| \n",
        "        30     | 0.001 - 0.01 |Relu| \n",
        "        40     | 0.001 - 0.01 |Relu| "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yc6uuMsaAmCH"
      },
      "source": [
        "def keraTunner1Dense(hp):\n",
        "  input = tf.keras.layers.Input(shape=(2,), name='Bat_Temp')\n",
        "  hiper_param = hp.Int('units', min_value=10, max_value=40, step=10)\n",
        "  dense = tf.keras.layers.Dense(units=hiper_param,activation='relu',name='camada_1')(input)\n",
        "  dense = tf.keras.layers.Dropout(0.2)(dense)\n",
        "  output = tf.keras.layers.Dense(1,name='salinidade')(dense)\n",
        "  modelo = tf.keras.Model(inputs=input, outputs=output)\n",
        "  taxa_apren =  hp.Choice('learning_rate', values=[0.001,0.002,0.003,0.004,0.005,0.006,0.007,0.008,0.009,0.01])\n",
        "  adan = tf.keras.optimizers.Adam(learning_rate=taxa_apren)\n",
        "  modelo.compile(optimizer=adan, loss='mse', metrics=['mae','mse'])\n",
        "  return modelo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xw9RzWP3BjRz"
      },
      "source": [
        "###5.2 Função para 2 camadas densa \n",
        "Neuronios|Taxa de aprendizagem|Função de aprendizagem\n",
        "---------------|--------------|----|\n",
        "     10-10     | 0.001 - 0.01 |Relu|    \n",
        "     20-20     | 0.001 - 0.01 |Relu| \n",
        "     30-30     | 0.001 - 0.01 |Relu| \n",
        "     40-30     | 0.001 - 0.01 |Relu| "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFrXb5CXBmJr"
      },
      "source": [
        "def keraTunner2Dense(hp):\n",
        "  input = tf.keras.layers.Input(shape=(2,), name='Bat_Temp')\n",
        "  hiper_param = hp.Int('units', min_value=10, max_value=40, step=10)\n",
        "  dense = tf.keras.layers.Dense(units=hiper_param,activation='relu',name='camada_1')(input)\n",
        "  dense = tf.keras.layers.Dense(units=hiper_param,activation='relu',name='camada_2')(dense)\n",
        "  dense = tf.keras.layers.Dropout(0.2)(dense)\n",
        "  output = tf.keras.layers.Dense(1,name='salinidade')(dense)\n",
        "  modelo = tf.keras.Model(inputs=input, outputs=output)\n",
        "  taxa_apren =  hp.Choice('learning_rate', values=[0.001,0.002,0.003,0.004,0.005,0.006,0.007,0.008,0.009,0.01])\n",
        "  adan = tf.keras.optimizers.Adam(learning_rate=taxa_apren)\n",
        "  modelo.compile(optimizer=adan, loss='mse', metrics=['mae','mse'])\n",
        "  return modelo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZPcgf4XBklJ"
      },
      "source": [
        "###5.3 Função para 3 camadas densa \n",
        "Neuronios|Taxa de aprendizagem|Função de aprendizagem\n",
        "---------------|--------------|----|\n",
        "  10-10-10     | 0.001 - 0.01 |Relu|    \n",
        "  20-20-20     | 0.001 - 0.01 |Relu| \n",
        "  30-30-30     | 0.001 - 0.01 |Relu| \n",
        "  40-40-40     | 0.001 - 0.01 |Relu| "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pso6bvbSBm6S"
      },
      "source": [
        "def keraTunner3Dense(hp):\n",
        "  input = tf.keras.layers.Input(shape=(2,), name='Bat_Temp')\n",
        "  hiper_param = hp.Int('units', min_value=10, max_value=40, step=10)\n",
        "  dense = tf.keras.layers.Dense(units=hiper_param,activation='relu',name='camada_1')(input)\n",
        "  dense = tf.keras.layers.Dense(units=hiper_param,activation='relu',name='camada_2')(dense)\n",
        "  dense = tf.keras.layers.Dense(units=hiper_param,activation='relu',name='camada_3')(dense)\n",
        "  dense = tf.keras.layers.Dropout(0.2)(dense)\n",
        "  output = tf.keras.layers.Dense(1,name='salinidade')(dense)\n",
        "  modelo = tf.keras.Model(inputs=input, outputs=output)\n",
        "  taxa_apren =  hp.Choice('learning_rate', values=[0.001,0.002,0.003,0.004,0.005,0.006,0.007,0.008,0.009,0.01])\n",
        "  adan = tf.keras.optimizers.Adam(learning_rate=taxa_apren)\n",
        "  modelo.compile(optimizer=adan, loss='mse', metrics=['mae','mse'])\n",
        "  return modelo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3k9LiBnBk9Z"
      },
      "source": [
        "###5.4 Função para 4 camadas densa \n",
        "Neuronios|Taxa de aprendizagem|Função de aprendizagem\n",
        "---------------|--------------|----|\n",
        "  10-10-10-10  | 0.001 - 0.01 |Relu|    \n",
        "  20-20-20-20  | 0.001 - 0.01 |Relu| \n",
        "  30-30-30-30  | 0.001 - 0.01 |Relu| \n",
        "  40-40-40-40  | 0.001 - 0.01 |Relu| "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUT8_vpDBoP6"
      },
      "source": [
        "def keraTunner4Dense(hp):\n",
        "  input = tf.keras.layers.Input(shape=(2,), name='Bat_Temp')\n",
        "  hiper_param = hp.Int('units', min_value=10, max_value=40, step=10)\n",
        "  dense = tf.keras.layers.Dense(units=hiper_param,activation='relu',name='camada_1')(input)\n",
        "  dense = tf.keras.layers.Dense(units=hiper_param,activation='relu',name='camada_2')(dense)\n",
        "  dense = tf.keras.layers.Dense(units=hiper_param,activation='relu',name='camada_3')(dense)\n",
        "  dense = tf.keras.layers.Dense(units=hiper_param,activation='relu',name='camada_4')(dense)\n",
        "  dense = tf.keras.layers.Dropout(0.2)(dense)\n",
        "  output = tf.keras.layers.Dense(1,name='salinidade')(dense)\n",
        "  modelo = tf.keras.Model(inputs=input, outputs=output)\n",
        "  taxa_apren =  hp.Choice('learning_rate', values=[0.001,0.002,0.003,0.004,0.005,0.006,0.007,0.008,0.009,0.01])\n",
        "  adan = tf.keras.optimizers.Adam(learning_rate=taxa_apren)\n",
        "  modelo.compile(optimizer=adan, loss='mse', metrics=['mae','mse'])\n",
        "  return modelo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iw2g0iQMBlMJ"
      },
      "source": [
        "###5.5 Função para 5 camadas densa \n",
        "Neuronios|Taxa de aprendizagem|Função de aprendizagem\n",
        "---------------|--------------|----|\n",
        "  10-10-10-10-10  | 0.001 - 0.01 |Relu|    \n",
        "  20-20-20-20-20  | 0.001 - 0.01 |Relu| \n",
        "  30-30-30-30-30  | 0.001 - 0.01 |Relu| \n",
        "  40-40-40-40-40  | 0.001 - 0.01 |Relu| "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyrkUtduBop6"
      },
      "source": [
        "def keraTunner5Dense(hp):\n",
        "  input = tf.keras.layers.Input(shape=(2,), name='Bat_Temp')\n",
        "  hiper_param = hp.Int('units', min_value=10, max_value=40, step=10)\n",
        "  dense = tf.keras.layers.Dense(units=hiper_param,activation='relu',name='camada_1')(input)\n",
        "  dense = tf.keras.layers.Dense(units=hiper_param,activation='relu',name='camada_2')(dense)\n",
        "  dense = tf.keras.layers.Dense(units=hiper_param,activation='relu',name='camada_3')(dense)\n",
        "  dense = tf.keras.layers.Dense(units=hiper_param,activation='relu',name='camada_4')(dense)\n",
        "  dense = tf.keras.layers.Dense(units=hiper_param,activation='relu',name='camada_5')(dense)\n",
        "  dense = tf.keras.layers.Dropout(0.2)(dense)\n",
        "  output = tf.keras.layers.Dense(1,name='salinidade')(dense)\n",
        "  modelo = tf.keras.Model(inputs=input, outputs=output)\n",
        "  taxa_apren =  hp.Choice('learning_rate', values=[0.001,0.002,0.003,0.004,0.005,0.006,0.007,0.008,0.009,0.01])\n",
        "  adan = tf.keras.optimizers.Adam(learning_rate=taxa_apren)\n",
        "  modelo.compile(optimizer=adan, loss='mse', metrics=['mae','mse'])\n",
        "  return modelo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6bLU9GQC3uN"
      },
      "source": [
        "##6. criar o tunner de parametros atraves da função Hyperband"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JozJp6frDrbi"
      },
      "source": [
        "###6.1 Tunner para 1 camada densa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5slsdRD4IAjC"
      },
      "source": [
        "####6.1.1 Tunner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzATf5VLIGZQ"
      },
      "source": [
        "tuner = kt.Hyperband(keraTunner1Dense,\n",
        "                     objective='val_mae',\n",
        "                     max_epochs=10,\n",
        "                     factor=3,\n",
        "                     directory='my_dir',\n",
        "                     project_name='salinidade')\n",
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "tuner.search(pr, sal,epochs=100,validation_split=0.2, callbacks=[stop_early])\n",
        "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmcYK4MnGgnh"
      },
      "source": [
        "####6.1.2 Resultado\n",
        "1. Numero de neuronios: 40    \n",
        "2. Taxa de aprendizagem: 0.003  \n",
        "3. Best MAE: 0.8738995790481567"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiiqhjlLHEWH"
      },
      "source": [
        "###6.2 Tunner para 2 camadas densa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-0TThdrIQei"
      },
      "source": [
        "####6.2.1 Tunner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BMeqm2JISqU"
      },
      "source": [
        "tuner = kt.Hyperband(keraTunner2Dense,\n",
        "                     objective='val_mae',\n",
        "                     max_epochs=10,\n",
        "                     factor=3,\n",
        "                     directory='my_dir',\n",
        "                     project_name='salinidade2')\n",
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "tuner.search(pr, sal,batch_size=120,epochs=100,validation_split=0.2, callbacks=[stop_early])\n",
        "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wY99dWxzHTiA"
      },
      "source": [
        "####6.2.2 Resultado\n",
        "1. Numero de neuronios: 30    \n",
        "2. Taxa de aprendizagem: 0.001 \n",
        "3. Best MAE: 0.8576666116714478"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzQB4MBNHE2X"
      },
      "source": [
        "###6.3 Tunner para 3 camadas densa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5_s4VYkIaEd"
      },
      "source": [
        "####6.3.1 Tunner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLZIBAoPIZNf"
      },
      "source": [
        "tuner = kt.Hyperband(keraTunner3Dense,\n",
        "                     objective='val_mae',\n",
        "                     max_epochs=10,\n",
        "                     factor=3,\n",
        "                     directory='my_dir',\n",
        "                     project_name='salinidade3')\n",
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "tuner.search(pr, sal,batch_size=120,epochs=100,validation_split=0.2, callbacks=[stop_early])\n",
        "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGLgnWD1HT4Y"
      },
      "source": [
        "####6.3.2 Resultado\n",
        "1. Numero de neuronios: 10    \n",
        "2. Taxa de aprendizagem: 0.009  \n",
        "3. Best MAE: 0.8453654050827026"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgWkcuYuHFLG"
      },
      "source": [
        "###6.4 Tunner para 4 camadas densa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rr3kQcw0Iqkb"
      },
      "source": [
        "####6.4.1 Tunner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69HLgn63ItHV"
      },
      "source": [
        "tuner = kt.Hyperband(keraTunner4Dense,\n",
        "                     objective='val_mae',\n",
        "                     max_epochs=10,\n",
        "                     factor=3,\n",
        "                     directory='my_dir',\n",
        "                     project_name='salinidade4')\n",
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "tuner.search(pr, sal,batch_size=120,epochs=100,validation_split=0.2, callbacks=[stop_early])\n",
        "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLaRylguHUT3"
      },
      "source": [
        "####6.4.2 Resultado\n",
        "1. Numero de neuronios: 20    \n",
        "2. Taxa de aprendizagem: 0.002  \n",
        "3. Best MAE: 0.8228367567062378"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpSD7sUtHFev"
      },
      "source": [
        "###6.5 Tunner para 5 camadas densa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzPSXzNWI1SH"
      },
      "source": [
        "####6.5.1 Tunner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5-WRsO0I3qv"
      },
      "source": [
        "tuner = kt.Hyperband(keraTunner5Dense,\n",
        "                     objective='val_mae',\n",
        "                     max_epochs=10,\n",
        "                     factor=3,\n",
        "                     directory='my_dir',\n",
        "                     project_name='salinidade5')\n",
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "tuner.search(pr, sal,batch_size=120,epochs=100,validation_split=0.2, callbacks=[stop_early])\n",
        "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6scKfztUHUqA"
      },
      "source": [
        "####6.5.2 Resultado\n",
        "1. Numero de neuronios: 10    \n",
        "2. Taxa de aprendizagem: 0.006\n",
        "3. Best MAE: 0.8543604612350464"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8btAjCu1QuVL"
      },
      "source": [
        "##7. fazer o treinamento de cada arquitetura com os dados de treinamento, depois realizar a escolha com os dados de teste"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeVmVOBMRlDh"
      },
      "source": [
        "###7.1 funções de treinamento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTJbKdgBRiZo"
      },
      "source": [
        "def denseLayer1():\n",
        "  input = tf.keras.layers.Input(shape=(2,), name='Bat_Temp')\n",
        "  dense = tf.keras.layers.Dense(units=40,activation='relu',name='camada_1')(input)\n",
        "  dense = tf.keras.layers.Dropout(0.2)(dense)\n",
        "  output = tf.keras.layers.Dense(1,name='salinidade')(dense)\n",
        "  modelo = tf.keras.Model(inputs=input, outputs=output)\n",
        "  adan = tf.keras.optimizers.Adam(learning_rate=0.003)\n",
        "  modelo.compile(optimizer=adan, loss='mse', metrics=['mae','mse'])\n",
        "  return modelo\n",
        "def denseLayer2():\n",
        "  input = tf.keras.layers.Input(shape=(2,), name='Bat_Temp')\n",
        "  dense = tf.keras.layers.Dense(units=30,activation='relu',name='camada_1')(input)\n",
        "  dense = tf.keras.layers.Dense(units=30,activation='relu',name='camada_2')(dense)\n",
        "  dense = tf.keras.layers.Dropout(0.2)(dense)\n",
        "  output = tf.keras.layers.Dense(1,name='salinidade')(dense)\n",
        "  modelo = tf.keras.Model(inputs=input, outputs=output)\n",
        "  adan = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "  modelo.compile(optimizer=adan, loss='mse', metrics=['mae','mse'])\n",
        "  return modelo\n",
        "def denseLayer3():\n",
        "  input = tf.keras.layers.Input(shape=(2,), name='Bat_Temp')\n",
        "  dense = tf.keras.layers.Dense(units=10,activation='relu',name='camada_1')(input)\n",
        "  dense = tf.keras.layers.Dense(units=10,activation='relu',name='camada_2')(dense)\n",
        "  dense = tf.keras.layers.Dense(units=10,activation='relu',name='camada_3')(dense)\n",
        "  dense = tf.keras.layers.Dropout(0.2)(dense)\n",
        "  output = tf.keras.layers.Dense(1,name='salinidade')(dense)\n",
        "  modelo = tf.keras.Model(inputs=input, outputs=output)\n",
        "  adan = tf.keras.optimizers.Adam(learning_rate=0.009)\n",
        "  modelo.compile(optimizer=adan, loss='mse', metrics=['mae','mse'])\n",
        "  return modelo\n",
        "def denseLayer4():\n",
        "  input = tf.keras.layers.Input(shape=(2,), name='Bat_Temp')\n",
        "  dense = tf.keras.layers.Dense(units=20,activation='relu',name='camada_1')(input)\n",
        "  dense = tf.keras.layers.Dense(units=20,activation='relu',name='camada_2')(dense)\n",
        "  dense = tf.keras.layers.Dense(units=20,activation='relu',name='camada_3')(dense)\n",
        "  dense = tf.keras.layers.Dense(units=20,activation='relu',name='camada_4')(dense)\n",
        "  dense = tf.keras.layers.Dropout(0.2)(dense)\n",
        "  output = tf.keras.layers.Dense(1,name='salinidade')(dense)\n",
        "  modelo = tf.keras.Model(inputs=input, outputs=output)\n",
        "  adan = tf.keras.optimizers.Adam(learning_rate=0.002)\n",
        "  modelo.compile(optimizer=adan, loss='mse', metrics=['mae','mse'])\n",
        "  return modelo\n",
        "def denseLayer5():\n",
        "  input = tf.keras.layers.Input(shape=(2,), name='Bat_Temp')\n",
        "  dense = tf.keras.layers.Dense(units=10,activation='relu',name='camada_1')(input)\n",
        "  dense = tf.keras.layers.Dense(units=10,activation='relu',name='camada_2')(dense)\n",
        "  dense = tf.keras.layers.Dense(units=10,activation='relu',name='camada_3')(dense)\n",
        "  dense = tf.keras.layers.Dense(units=10,activation='relu',name='camada_4')(dense)\n",
        "  dense = tf.keras.layers.Dense(units=10,activation='relu',name='camada_5')(dense)\n",
        "  dense = tf.keras.layers.Dropout(0.2)(dense)\n",
        "  output = tf.keras.layers.Dense(1,name='salinidade')(dense)\n",
        "  modelo = tf.keras.Model(inputs=input, outputs=output)\n",
        "  adan = tf.keras.optimizers.Adam(learning_rate=0.006)\n",
        "  modelo.compile(optimizer=adan, loss='mse', metrics=['mae','mse'])\n",
        "  return modelo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCFdEhT1RtGU"
      },
      "source": [
        "###7.2 treinar os modelos com dados de treino e avaliar com dados de teste"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JV_9w7WYSLAm"
      },
      "source": [
        "####7.1 modelo com 1 camada"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSEm9ItvS1pk",
        "outputId": "55847ea1-8d24-497d-c1a2-bd10676cdc8c"
      },
      "source": [
        "modelo_1C = denseLayer1()\n",
        "best_epoch = modelo_1C.fit(pr_treino,sal_treino,epochs=100,batch_size=120)\n",
        "val_acc_per_epoch = best_epoch.history['mae']\n",
        "best_epoch = val_acc_per_epoch.index(min(val_acc_per_epoch))+1\n",
        "modelo_1C.fit(pr_treino,sal_treino,epochs=best_epoch,batch_size=120)\n",
        "modelo_1C.evaluate(pr_teste,sal_teste)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 37.0090 - mae: 3.9935 - mse: 37.0090\n",
            "Epoch 2/100\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 8.3028 - mae: 1.8620 - mse: 8.3028\n",
            "Epoch 3/100\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.6727 - mae: 1.0162 - mse: 4.6727\n",
            "Epoch 4/100\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4951 - mae: 0.9355 - mse: 4.4951\n",
            "Epoch 5/100\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4925 - mae: 0.9337 - mse: 4.4925\n",
            "Epoch 6/100\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4918 - mae: 0.9326 - mse: 4.4918\n",
            "Epoch 7/100\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4916 - mae: 0.9323 - mse: 4.4916\n",
            "Epoch 8/100\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4917 - mae: 0.9314 - mse: 4.4917\n",
            "Epoch 9/100\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4923 - mae: 0.9311 - mse: 4.4923\n",
            "Epoch 10/100\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4917 - mae: 0.9306 - mse: 4.4917\n",
            "Epoch 11/100\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4905 - mae: 0.9304 - mse: 4.4905\n",
            "Epoch 12/100\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4907 - mae: 0.9300 - mse: 4.4907\n",
            "Epoch 13/100\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4904 - mae: 0.9298 - mse: 4.4904\n",
            "Epoch 14/100\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4898 - mae: 0.9296 - mse: 4.4898\n",
            "Epoch 15/100\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4895 - mae: 0.9296 - mse: 4.4895\n",
            "Epoch 16/100\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4893 - mae: 0.9293 - mse: 4.4893\n",
            "Epoch 17/100\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4897 - mae: 0.9293 - mse: 4.4897\n",
            "Epoch 18/100\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4891 - mae: 0.9293 - mse: 4.4891\n",
            "Epoch 19/100\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4886 - mae: 0.9292 - mse: 4.4886\n",
            "Epoch 20/100\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4887 - mae: 0.9292 - mse: 4.4887\n",
            "Epoch 21/100\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4889 - mae: 0.9290 - mse: 4.4889\n",
            "Epoch 22/100\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4890 - mae: 0.9291 - mse: 4.4890\n",
            "Epoch 23/100\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4890 - mae: 0.9289 - mse: 4.4890\n",
            "Epoch 24/100\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4884 - mae: 0.9291 - mse: 4.4884\n",
            "Epoch 25/100\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4882 - mae: 0.9287 - mse: 4.4882\n",
            "Epoch 26/100\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4887 - mae: 0.9288 - mse: 4.4887\n",
            "Epoch 27/100\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4878 - mae: 0.9287 - mse: 4.4878\n",
            "Epoch 28/100\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4882 - mae: 0.9285 - mse: 4.4882\n",
            "Epoch 29/100\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4875 - mae: 0.9284 - mse: 4.4875\n",
            "Epoch 30/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.4883 - mae: 0.9286 - mse: 4.4883\n",
            "Epoch 31/100\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4878 - mae: 0.9287 - mse: 4.4878\n",
            "Epoch 32/100\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4870 - mae: 0.9283 - mse: 4.4870\n",
            "Epoch 33/100\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4876 - mae: 0.9285 - mse: 4.4876\n",
            "Epoch 34/100\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4866 - mae: 0.9284 - mse: 4.4866\n",
            "Epoch 35/100\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4870 - mae: 0.9282 - mse: 4.4870\n",
            "Epoch 36/100\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4873 - mae: 0.9283 - mse: 4.4873\n",
            "Epoch 37/100\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4879 - mae: 0.9285 - mse: 4.4879\n",
            "Epoch 38/100\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4870 - mae: 0.9285 - mse: 4.4870\n",
            "Epoch 39/100\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4869 - mae: 0.9284 - mse: 4.4869\n",
            "Epoch 40/100\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4875 - mae: 0.9284 - mse: 4.4875\n",
            "Epoch 41/100\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4871 - mae: 0.9282 - mse: 4.4871\n",
            "Epoch 42/100\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4866 - mae: 0.9284 - mse: 4.4866\n",
            "Epoch 43/100\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4867 - mae: 0.9284 - mse: 4.4867\n",
            "Epoch 44/100\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4870 - mae: 0.9279 - mse: 4.4870\n",
            "Epoch 45/100\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4875 - mae: 0.9282 - mse: 4.4875\n",
            "Epoch 46/100\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4871 - mae: 0.9282 - mse: 4.4871\n",
            "Epoch 47/100\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4872 - mae: 0.9279 - mse: 4.4872\n",
            "Epoch 48/100\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4865 - mae: 0.9280 - mse: 4.4865\n",
            "Epoch 49/100\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4864 - mae: 0.9279 - mse: 4.4864\n",
            "Epoch 50/100\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4871 - mae: 0.9281 - mse: 4.4871\n",
            "Epoch 51/100\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4869 - mae: 0.9279 - mse: 4.4869\n",
            "Epoch 52/100\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4867 - mae: 0.9282 - mse: 4.4867\n",
            "Epoch 53/100\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4866 - mae: 0.9281 - mse: 4.4866\n",
            "Epoch 54/100\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4864 - mae: 0.9278 - mse: 4.4864\n",
            "Epoch 55/100\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4865 - mae: 0.9282 - mse: 4.4865\n",
            "Epoch 56/100\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4870 - mae: 0.9279 - mse: 4.4870\n",
            "Epoch 57/100\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4868 - mae: 0.9278 - mse: 4.4868\n",
            "Epoch 58/100\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4877 - mae: 0.9280 - mse: 4.4877\n",
            "Epoch 59/100\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4861 - mae: 0.9282 - mse: 4.4861\n",
            "Epoch 60/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.4870 - mae: 0.9275 - mse: 4.4870\n",
            "Epoch 61/100\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4864 - mae: 0.9280 - mse: 4.4864\n",
            "Epoch 62/100\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4864 - mae: 0.9277 - mse: 4.4864\n",
            "Epoch 63/100\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4863 - mae: 0.9280 - mse: 4.4863\n",
            "Epoch 64/100\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4865 - mae: 0.9280 - mse: 4.4865\n",
            "Epoch 65/100\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4870 - mae: 0.9278 - mse: 4.4870\n",
            "Epoch 66/100\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4872 - mae: 0.9278 - mse: 4.4872\n",
            "Epoch 67/100\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4869 - mae: 0.9278 - mse: 4.4869\n",
            "Epoch 68/100\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4866 - mae: 0.9276 - mse: 4.4866\n",
            "Epoch 69/100\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4861 - mae: 0.9278 - mse: 4.4861\n",
            "Epoch 70/100\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4860 - mae: 0.9278 - mse: 4.4860\n",
            "Epoch 71/100\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4859 - mae: 0.9278 - mse: 4.4859\n",
            "Epoch 72/100\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4864 - mae: 0.9279 - mse: 4.4864\n",
            "Epoch 73/100\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4862 - mae: 0.9278 - mse: 4.4862\n",
            "Epoch 74/100\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4864 - mae: 0.9275 - mse: 4.4864\n",
            "Epoch 75/100\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4857 - mae: 0.9278 - mse: 4.4857\n",
            "Epoch 76/100\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4862 - mae: 0.9280 - mse: 4.4862\n",
            "Epoch 77/100\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4867 - mae: 0.9277 - mse: 4.4867\n",
            "Epoch 78/100\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4869 - mae: 0.9280 - mse: 4.4869\n",
            "Epoch 79/100\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4865 - mae: 0.9279 - mse: 4.4865\n",
            "Epoch 80/100\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4862 - mae: 0.9277 - mse: 4.4862\n",
            "Epoch 81/100\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4856 - mae: 0.9278 - mse: 4.4856\n",
            "Epoch 82/100\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4869 - mae: 0.9279 - mse: 4.4869\n",
            "Epoch 83/100\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4863 - mae: 0.9276 - mse: 4.4863\n",
            "Epoch 84/100\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4862 - mae: 0.9278 - mse: 4.4862\n",
            "Epoch 85/100\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4855 - mae: 0.9278 - mse: 4.4855\n",
            "Epoch 86/100\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4859 - mae: 0.9277 - mse: 4.4859\n",
            "Epoch 87/100\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4867 - mae: 0.9276 - mse: 4.4867\n",
            "Epoch 88/100\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4863 - mae: 0.9277 - mse: 4.4863\n",
            "Epoch 89/100\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4864 - mae: 0.9276 - mse: 4.4864\n",
            "Epoch 90/100\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4857 - mae: 0.9277 - mse: 4.4857\n",
            "Epoch 91/100\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4869 - mae: 0.9276 - mse: 4.4869\n",
            "Epoch 92/100\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4855 - mae: 0.9276 - mse: 4.4855\n",
            "Epoch 93/100\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4851 - mae: 0.9275 - mse: 4.4851\n",
            "Epoch 94/100\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4861 - mae: 0.9277 - mse: 4.4861\n",
            "Epoch 95/100\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4862 - mae: 0.9277 - mse: 4.4862\n",
            "Epoch 96/100\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4857 - mae: 0.9278 - mse: 4.4857\n",
            "Epoch 97/100\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4857 - mae: 0.9275 - mse: 4.4857\n",
            "Epoch 98/100\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4860 - mae: 0.9274 - mse: 4.4860\n",
            "Epoch 99/100\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4857 - mae: 0.9276 - mse: 4.4857\n",
            "Epoch 100/100\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4863 - mae: 0.9275 - mse: 4.4863\n",
            "Epoch 1/98\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4860 - mae: 0.9276 - mse: 4.4860\n",
            "Epoch 2/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4850 - mae: 0.9276 - mse: 4.4850\n",
            "Epoch 3/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4864 - mae: 0.9276 - mse: 4.4864\n",
            "Epoch 4/98\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4863 - mae: 0.9274 - mse: 4.4863\n",
            "Epoch 5/98\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4865 - mae: 0.9276 - mse: 4.4865\n",
            "Epoch 6/98\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4852 - mae: 0.9275 - mse: 4.4852\n",
            "Epoch 7/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4856 - mae: 0.9277 - mse: 4.4856\n",
            "Epoch 8/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4854 - mae: 0.9276 - mse: 4.4854\n",
            "Epoch 9/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4858 - mae: 0.9275 - mse: 4.4858\n",
            "Epoch 10/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4859 - mae: 0.9277 - mse: 4.4859\n",
            "Epoch 11/98\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4854 - mae: 0.9273 - mse: 4.4854\n",
            "Epoch 12/98\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4858 - mae: 0.9277 - mse: 4.4858\n",
            "Epoch 13/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4860 - mae: 0.9274 - mse: 4.4860\n",
            "Epoch 14/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4861 - mae: 0.9277 - mse: 4.4861\n",
            "Epoch 15/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4856 - mae: 0.9275 - mse: 4.4856\n",
            "Epoch 16/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4851 - mae: 0.9275 - mse: 4.4851\n",
            "Epoch 17/98\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4860 - mae: 0.9274 - mse: 4.4860\n",
            "Epoch 18/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4861 - mae: 0.9274 - mse: 4.4861\n",
            "Epoch 19/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4857 - mae: 0.9273 - mse: 4.4857\n",
            "Epoch 20/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4856 - mae: 0.9275 - mse: 4.4856\n",
            "Epoch 21/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4863 - mae: 0.9277 - mse: 4.4863\n",
            "Epoch 22/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4854 - mae: 0.9274 - mse: 4.4854\n",
            "Epoch 23/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4858 - mae: 0.9275 - mse: 4.4858\n",
            "Epoch 24/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4853 - mae: 0.9272 - mse: 4.4853\n",
            "Epoch 25/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4854 - mae: 0.9273 - mse: 4.4854\n",
            "Epoch 26/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4855 - mae: 0.9276 - mse: 4.4855\n",
            "Epoch 27/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4858 - mae: 0.9276 - mse: 4.4858\n",
            "Epoch 28/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4854 - mae: 0.9275 - mse: 4.4854\n",
            "Epoch 29/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4861 - mae: 0.9275 - mse: 4.4861\n",
            "Epoch 30/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4861 - mae: 0.9276 - mse: 4.4861\n",
            "Epoch 31/98\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4864 - mae: 0.9275 - mse: 4.4864\n",
            "Epoch 32/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4863 - mae: 0.9275 - mse: 4.4863\n",
            "Epoch 33/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4852 - mae: 0.9275 - mse: 4.4852\n",
            "Epoch 34/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4853 - mae: 0.9272 - mse: 4.4853\n",
            "Epoch 35/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4862 - mae: 0.9277 - mse: 4.4862\n",
            "Epoch 36/98\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4856 - mae: 0.9273 - mse: 4.4856\n",
            "Epoch 37/98\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4855 - mae: 0.9274 - mse: 4.4855\n",
            "Epoch 38/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4857 - mae: 0.9276 - mse: 4.4857\n",
            "Epoch 39/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4857 - mae: 0.9274 - mse: 4.4857\n",
            "Epoch 40/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4849 - mae: 0.9273 - mse: 4.4849\n",
            "Epoch 41/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4863 - mae: 0.9277 - mse: 4.4863\n",
            "Epoch 42/98\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4860 - mae: 0.9274 - mse: 4.4860\n",
            "Epoch 43/98\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4860 - mae: 0.9273 - mse: 4.4860\n",
            "Epoch 44/98\n",
            "10093/10093 [==============================] - 16s 2ms/step - loss: 4.4852 - mae: 0.9274 - mse: 4.4852\n",
            "Epoch 45/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4851 - mae: 0.9273 - mse: 4.4851\n",
            "Epoch 46/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4860 - mae: 0.9277 - mse: 4.4860\n",
            "Epoch 47/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4865 - mae: 0.9273 - mse: 4.4865\n",
            "Epoch 48/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4858 - mae: 0.9275 - mse: 4.4858\n",
            "Epoch 49/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4852 - mae: 0.9275 - mse: 4.4852\n",
            "Epoch 50/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4851 - mae: 0.9275 - mse: 4.4851\n",
            "Epoch 51/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4858 - mae: 0.9275 - mse: 4.4858\n",
            "Epoch 52/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4858 - mae: 0.9274 - mse: 4.4858\n",
            "Epoch 53/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4857 - mae: 0.9275 - mse: 4.4857\n",
            "Epoch 54/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4857 - mae: 0.9274 - mse: 4.4857\n",
            "Epoch 55/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4852 - mae: 0.9273 - mse: 4.4852\n",
            "Epoch 56/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4849 - mae: 0.9273 - mse: 4.4849\n",
            "Epoch 57/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4857 - mae: 0.9275 - mse: 4.4857\n",
            "Epoch 58/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4859 - mae: 0.9275 - mse: 4.4859\n",
            "Epoch 59/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4856 - mae: 0.9275 - mse: 4.4856\n",
            "Epoch 60/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4853 - mae: 0.9274 - mse: 4.4853\n",
            "Epoch 61/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4854 - mae: 0.9276 - mse: 4.4854\n",
            "Epoch 62/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4852 - mae: 0.9275 - mse: 4.4852\n",
            "Epoch 63/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4848 - mae: 0.9275 - mse: 4.4848\n",
            "Epoch 64/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4853 - mae: 0.9273 - mse: 4.4853\n",
            "Epoch 65/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4854 - mae: 0.9273 - mse: 4.4854\n",
            "Epoch 66/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4857 - mae: 0.9273 - mse: 4.4857\n",
            "Epoch 67/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4859 - mae: 0.9273 - mse: 4.4859\n",
            "Epoch 68/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4860 - mae: 0.9276 - mse: 4.4860\n",
            "Epoch 69/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4861 - mae: 0.9272 - mse: 4.4861\n",
            "Epoch 70/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4860 - mae: 0.9275 - mse: 4.4860\n",
            "Epoch 71/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4864 - mae: 0.9273 - mse: 4.4864\n",
            "Epoch 72/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4856 - mae: 0.9276 - mse: 4.4856\n",
            "Epoch 73/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4854 - mae: 0.9272 - mse: 4.4854\n",
            "Epoch 74/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4858 - mae: 0.9275 - mse: 4.4858\n",
            "Epoch 75/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4855 - mae: 0.9276 - mse: 4.4855\n",
            "Epoch 76/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4854 - mae: 0.9273 - mse: 4.4854\n",
            "Epoch 77/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4852 - mae: 0.9275 - mse: 4.4852\n",
            "Epoch 78/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4855 - mae: 0.9274 - mse: 4.4855\n",
            "Epoch 79/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4863 - mae: 0.9275 - mse: 4.4863\n",
            "Epoch 80/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4857 - mae: 0.9274 - mse: 4.4857\n",
            "Epoch 81/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4857 - mae: 0.9274 - mse: 4.4857\n",
            "Epoch 82/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4861 - mae: 0.9273 - mse: 4.4861\n",
            "Epoch 83/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4855 - mae: 0.9274 - mse: 4.4855\n",
            "Epoch 84/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4864 - mae: 0.9273 - mse: 4.4864\n",
            "Epoch 85/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4855 - mae: 0.9273 - mse: 4.4855\n",
            "Epoch 86/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4861 - mae: 0.9272 - mse: 4.4861\n",
            "Epoch 87/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4855 - mae: 0.9274 - mse: 4.4855\n",
            "Epoch 88/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4862 - mae: 0.9273 - mse: 4.4862\n",
            "Epoch 89/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4855 - mae: 0.9272 - mse: 4.4855\n",
            "Epoch 90/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4850 - mae: 0.9273 - mse: 4.4850\n",
            "Epoch 91/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4857 - mae: 0.9274 - mse: 4.4857\n",
            "Epoch 92/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4851 - mae: 0.9272 - mse: 4.4851\n",
            "Epoch 93/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4860 - mae: 0.9273 - mse: 4.4860\n",
            "Epoch 94/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4855 - mae: 0.9272 - mse: 4.4855\n",
            "Epoch 95/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4855 - mae: 0.9274 - mse: 4.4855\n",
            "Epoch 96/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4857 - mae: 0.9273 - mse: 4.4857\n",
            "Epoch 97/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4852 - mae: 0.9273 - mse: 4.4852\n",
            "Epoch 98/98\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4860 - mae: 0.9274 - mse: 4.4860\n",
            "16221/16221 [==============================] - 21s 1ms/step - loss: 4.6284 - mae: 0.9254 - mse: 4.6284\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4.628376007080078, 0.9253870248794556, 4.628376007080078]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kuhhDCQSP6U"
      },
      "source": [
        "####7.2 modelo com 2 camada"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bP_B3sw3TGoZ",
        "outputId": "1a3e2e9c-551a-431c-87da-cd51bd7fbdb2"
      },
      "source": [
        "modelo_2C = denseLayer2()\n",
        "best_epoch = modelo_2C.fit(pr_treino,sal_treino,epochs=100,batch_size=120)\n",
        "val_acc_per_epoch = best_epoch.history['mae']\n",
        "best_epoch = val_acc_per_epoch.index(min(val_acc_per_epoch))+1\n",
        "modelo_2C.fit(pr_treino,sal_treino,epochs=best_epoch,batch_size=120)\n",
        "modelo_2C.evaluate(pr_teste,sal_teste)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 53.8137 - mae: 5.2540 - mse: 53.8137\n",
            "Epoch 2/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 23.7483 - mae: 3.7545 - mse: 23.7483\n",
            "Epoch 3/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 13.9083 - mae: 2.7294 - mse: 13.9083\n",
            "Epoch 4/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 7.7179 - mae: 1.7945 - mse: 7.7179\n",
            "Epoch 5/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 5.2097 - mae: 1.2082 - mse: 5.2097\n",
            "Epoch 6/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.5433 - mae: 0.9784 - mse: 4.5433\n",
            "Epoch 7/100\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.4047 - mae: 0.9175 - mse: 4.4047\n",
            "Epoch 8/100\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.3931 - mae: 0.9116 - mse: 4.3931\n",
            "Epoch 9/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.3893 - mae: 0.9113 - mse: 4.3893\n",
            "Epoch 10/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.3887 - mae: 0.9109 - mse: 4.3887\n",
            "Epoch 11/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.3866 - mae: 0.9106 - mse: 4.3866\n",
            "Epoch 12/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.3869 - mae: 0.9106 - mse: 4.3869\n",
            "Epoch 13/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.3870 - mae: 0.9100 - mse: 4.3870\n",
            "Epoch 14/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.3857 - mae: 0.9102 - mse: 4.3857\n",
            "Epoch 15/100\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.3849 - mae: 0.9099 - mse: 4.3849\n",
            "Epoch 16/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.3802 - mae: 0.9098 - mse: 4.3802\n",
            "Epoch 17/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.3800 - mae: 0.9092 - mse: 4.3800\n",
            "Epoch 18/100\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.3795 - mae: 0.9088 - mse: 4.3795\n",
            "Epoch 19/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.3778 - mae: 0.9089 - mse: 4.3778\n",
            "Epoch 20/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.3775 - mae: 0.9093 - mse: 4.3775\n",
            "Epoch 21/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.3766 - mae: 0.9085 - mse: 4.3766\n",
            "Epoch 22/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.3757 - mae: 0.9076 - mse: 4.3757\n",
            "Epoch 23/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.3741 - mae: 0.9081 - mse: 4.3741\n",
            "Epoch 24/100\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.3636 - mae: 0.9070 - mse: 4.3636\n",
            "Epoch 25/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.3497 - mae: 0.9001 - mse: 4.3497\n",
            "Epoch 26/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.3465 - mae: 0.8969 - mse: 4.3465\n",
            "Epoch 27/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.3461 - mae: 0.8961 - mse: 4.3461\n",
            "Epoch 28/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.3431 - mae: 0.8949 - mse: 4.3431\n",
            "Epoch 29/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.3383 - mae: 0.8935 - mse: 4.3383\n",
            "Epoch 30/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.3248 - mae: 0.8888 - mse: 4.3248\n",
            "Epoch 31/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.3189 - mae: 0.8869 - mse: 4.3189\n",
            "Epoch 32/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.3146 - mae: 0.8865 - mse: 4.3146\n",
            "Epoch 33/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.3114 - mae: 0.8864 - mse: 4.3114\n",
            "Epoch 34/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.3132 - mae: 0.8863 - mse: 4.3132\n",
            "Epoch 35/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.3118 - mae: 0.8860 - mse: 4.3118\n",
            "Epoch 36/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.3131 - mae: 0.8861 - mse: 4.3131\n",
            "Epoch 37/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.3100 - mae: 0.8860 - mse: 4.3100\n",
            "Epoch 38/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.3085 - mae: 0.8855 - mse: 4.3085\n",
            "Epoch 39/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.3086 - mae: 0.8854 - mse: 4.3086\n",
            "Epoch 40/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.3070 - mae: 0.8861 - mse: 4.3070\n",
            "Epoch 41/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.3090 - mae: 0.8854 - mse: 4.3090\n",
            "Epoch 42/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.3062 - mae: 0.8855 - mse: 4.3062\n",
            "Epoch 43/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.3055 - mae: 0.8859 - mse: 4.3055\n",
            "Epoch 44/100\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.3089 - mae: 0.8855 - mse: 4.3089\n",
            "Epoch 45/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.3056 - mae: 0.8862 - mse: 4.3056\n",
            "Epoch 46/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.3073 - mae: 0.8858 - mse: 4.3073\n",
            "Epoch 47/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.3097 - mae: 0.8861 - mse: 4.3097\n",
            "Epoch 48/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.3076 - mae: 0.8862 - mse: 4.3076\n",
            "Epoch 49/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.3067 - mae: 0.8860 - mse: 4.3067\n",
            "Epoch 50/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.3063 - mae: 0.8861 - mse: 4.3063\n",
            "Epoch 51/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.3050 - mae: 0.8855 - mse: 4.3050\n",
            "Epoch 52/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.3051 - mae: 0.8858 - mse: 4.3051\n",
            "Epoch 53/100\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.3072 - mae: 0.8862 - mse: 4.3072\n",
            "Epoch 54/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.3066 - mae: 0.8860 - mse: 4.3066\n",
            "Epoch 55/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.3070 - mae: 0.8858 - mse: 4.3070\n",
            "Epoch 56/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.3042 - mae: 0.8864 - mse: 4.3042\n",
            "Epoch 57/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.3042 - mae: 0.8862 - mse: 4.3042\n",
            "Epoch 58/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2993 - mae: 0.8856 - mse: 4.2993\n",
            "Epoch 59/100\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.3038 - mae: 0.8856 - mse: 4.3038\n",
            "Epoch 60/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.3020 - mae: 0.8863 - mse: 4.3020\n",
            "Epoch 61/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.3046 - mae: 0.8861 - mse: 4.3046\n",
            "Epoch 62/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.3044 - mae: 0.8861 - mse: 4.3044\n",
            "Epoch 63/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.3014 - mae: 0.8858 - mse: 4.3014\n",
            "Epoch 64/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.3010 - mae: 0.8862 - mse: 4.3010\n",
            "Epoch 65/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.3021 - mae: 0.8857 - mse: 4.3021\n",
            "Epoch 66/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2996 - mae: 0.8858 - mse: 4.2996\n",
            "Epoch 67/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.3007 - mae: 0.8859 - mse: 4.3007\n",
            "Epoch 68/100\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.3045 - mae: 0.8862 - mse: 4.3045\n",
            "Epoch 69/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.3018 - mae: 0.8863 - mse: 4.3018\n",
            "Epoch 70/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.3016 - mae: 0.8859 - mse: 4.3016\n",
            "Epoch 71/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2974 - mae: 0.8860 - mse: 4.2974\n",
            "Epoch 72/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2997 - mae: 0.8862 - mse: 4.2997\n",
            "Epoch 73/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.3020 - mae: 0.8862 - mse: 4.3020\n",
            "Epoch 74/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.3002 - mae: 0.8858 - mse: 4.3002\n",
            "Epoch 75/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.3017 - mae: 0.8861 - mse: 4.3017\n",
            "Epoch 76/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.3021 - mae: 0.8857 - mse: 4.3021\n",
            "Epoch 77/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2991 - mae: 0.8859 - mse: 4.2991\n",
            "Epoch 78/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2996 - mae: 0.8861 - mse: 4.2996\n",
            "Epoch 79/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2980 - mae: 0.8858 - mse: 4.2980\n",
            "Epoch 80/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.3003 - mae: 0.8859 - mse: 4.3003\n",
            "Epoch 81/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2999 - mae: 0.8863 - mse: 4.2999\n",
            "Epoch 82/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2994 - mae: 0.8858 - mse: 4.2994\n",
            "Epoch 83/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2988 - mae: 0.8864 - mse: 4.2988\n",
            "Epoch 84/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.3002 - mae: 0.8860 - mse: 4.3002\n",
            "Epoch 85/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2978 - mae: 0.8857 - mse: 4.2978\n",
            "Epoch 86/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2978 - mae: 0.8858 - mse: 4.2978\n",
            "Epoch 87/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2968 - mae: 0.8860 - mse: 4.2968\n",
            "Epoch 88/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2972 - mae: 0.8863 - mse: 4.2972\n",
            "Epoch 89/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2989 - mae: 0.8859 - mse: 4.2989\n",
            "Epoch 90/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2987 - mae: 0.8856 - mse: 4.2987\n",
            "Epoch 91/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.3000 - mae: 0.8863 - mse: 4.3000\n",
            "Epoch 92/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2989 - mae: 0.8856 - mse: 4.2989\n",
            "Epoch 93/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2968 - mae: 0.8860 - mse: 4.2968\n",
            "Epoch 94/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2977 - mae: 0.8855 - mse: 4.2977\n",
            "Epoch 95/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2983 - mae: 0.8864 - mse: 4.2983\n",
            "Epoch 96/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2989 - mae: 0.8859 - mse: 4.2989\n",
            "Epoch 97/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2980 - mae: 0.8861 - mse: 4.2980\n",
            "Epoch 98/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2968 - mae: 0.8855 - mse: 4.2968\n",
            "Epoch 99/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2970 - mae: 0.8865 - mse: 4.2970\n",
            "Epoch 100/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2953 - mae: 0.8859 - mse: 4.2953\n",
            "Epoch 1/39\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2956 - mae: 0.8858 - mse: 4.2956\n",
            "Epoch 2/39\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2982 - mae: 0.8858 - mse: 4.2982\n",
            "Epoch 3/39\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2948 - mae: 0.8858 - mse: 4.2948\n",
            "Epoch 4/39\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2985 - mae: 0.8866 - mse: 4.2985\n",
            "Epoch 5/39\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2993 - mae: 0.8857 - mse: 4.2993\n",
            "Epoch 6/39\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2977 - mae: 0.8858 - mse: 4.2977\n",
            "Epoch 7/39\n",
            "10093/10093 [==============================] - 20s 2ms/step - loss: 4.2969 - mae: 0.8855 - mse: 4.2969\n",
            "Epoch 8/39\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.2948 - mae: 0.8855 - mse: 4.2948\n",
            "Epoch 9/39\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2971 - mae: 0.8862 - mse: 4.2971\n",
            "Epoch 10/39\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2930 - mae: 0.8858 - mse: 4.2930\n",
            "Epoch 11/39\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2948 - mae: 0.8860 - mse: 4.2948\n",
            "Epoch 12/39\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2947 - mae: 0.8862 - mse: 4.2947\n",
            "Epoch 13/39\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2934 - mae: 0.8860 - mse: 4.2934\n",
            "Epoch 14/39\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2971 - mae: 0.8857 - mse: 4.2971\n",
            "Epoch 15/39\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2950 - mae: 0.8857 - mse: 4.2950\n",
            "Epoch 16/39\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2948 - mae: 0.8855 - mse: 4.2948\n",
            "Epoch 17/39\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2891 - mae: 0.8846 - mse: 4.2891\n",
            "Epoch 18/39\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.2879 - mae: 0.8844 - mse: 4.2879\n",
            "Epoch 19/39\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2898 - mae: 0.8844 - mse: 4.2898\n",
            "Epoch 20/39\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.2742 - mae: 0.8809 - mse: 4.2742\n",
            "Epoch 21/39\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.2612 - mae: 0.8768 - mse: 4.2612\n",
            "Epoch 22/39\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.2522 - mae: 0.8736 - mse: 4.2522\n",
            "Epoch 23/39\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.2476 - mae: 0.8709 - mse: 4.2476\n",
            "Epoch 24/39\n",
            "10093/10093 [==============================] - 20s 2ms/step - loss: 4.2455 - mae: 0.8699 - mse: 4.2455\n",
            "Epoch 25/39\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.2439 - mae: 0.8697 - mse: 4.2439\n",
            "Epoch 26/39\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.2403 - mae: 0.8684 - mse: 4.2403\n",
            "Epoch 27/39\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2390 - mae: 0.8682 - mse: 4.2390\n",
            "Epoch 28/39\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.2328 - mae: 0.8683 - mse: 4.2328\n",
            "Epoch 29/39\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2320 - mae: 0.8683 - mse: 4.2320\n",
            "Epoch 30/39\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.2282 - mae: 0.8674 - mse: 4.2282\n",
            "Epoch 31/39\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.2296 - mae: 0.8672 - mse: 4.2296\n",
            "Epoch 32/39\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.2285 - mae: 0.8673 - mse: 4.2285\n",
            "Epoch 33/39\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.2290 - mae: 0.8678 - mse: 4.2290\n",
            "Epoch 34/39\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.2267 - mae: 0.8671 - mse: 4.2267\n",
            "Epoch 35/39\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.2334 - mae: 0.8674 - mse: 4.2334\n",
            "Epoch 36/39\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.2268 - mae: 0.8675 - mse: 4.2268\n",
            "Epoch 37/39\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.2309 - mae: 0.8673 - mse: 4.2309\n",
            "Epoch 38/39\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.2273 - mae: 0.8668 - mse: 4.2273\n",
            "Epoch 39/39\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.2261 - mae: 0.8669 - mse: 4.2261\n",
            "16221/16221 [==============================] - 23s 1ms/step - loss: 4.3379 - mae: 0.8457 - mse: 4.3379\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4.3379316329956055, 0.8456872701644897, 4.3379316329956055]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-iOPxtoSQMr"
      },
      "source": [
        "####7.3 modelo com 3 camada"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBrJq3VUTJeH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca9b9aab-8e41-4d3b-b86c-638755311ef9"
      },
      "source": [
        "modelo_3C = denseLayer3()\n",
        "best_epoch = modelo_3C.fit(pr_treino,sal_treino,epochs=100,batch_size=120)\n",
        "val_acc_per_epoch = best_epoch.history['mae']\n",
        "best_epoch = val_acc_per_epoch.index(min(val_acc_per_epoch))+1\n",
        "modelo_3C.fit(pr_treino,sal_treino,epochs=best_epoch,batch_size=120)\n",
        "modelo_3C.evaluate(pr_teste,sal_teste)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 30.2575 - mae: 3.5001 - mse: 30.2575\n",
            "Epoch 2/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.4791 - mae: 0.9352 - mse: 4.4791\n",
            "Epoch 3/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.4634 - mae: 0.9347 - mse: 4.4634\n",
            "Epoch 4/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.4654 - mae: 0.9353 - mse: 4.4654\n",
            "Epoch 5/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.4665 - mae: 0.9355 - mse: 4.4665\n",
            "Epoch 6/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.4651 - mae: 0.9351 - mse: 4.4651\n",
            "Epoch 7/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.4663 - mae: 0.9349 - mse: 4.4663\n",
            "Epoch 8/100\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.4657 - mae: 0.9353 - mse: 4.4657\n",
            "Epoch 9/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.4636 - mae: 0.9352 - mse: 4.4636\n",
            "Epoch 10/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.4654 - mae: 0.9353 - mse: 4.4654\n",
            "Epoch 11/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.4641 - mae: 0.9350 - mse: 4.4641\n",
            "Epoch 12/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.4652 - mae: 0.9353 - mse: 4.4652\n",
            "Epoch 13/100\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.4642 - mae: 0.9349 - mse: 4.4642\n",
            "Epoch 14/100\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.4648 - mae: 0.9353 - mse: 4.4648\n",
            "Epoch 15/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.4640 - mae: 0.9348 - mse: 4.4640\n",
            "Epoch 16/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.4639 - mae: 0.9353 - mse: 4.4639\n",
            "Epoch 17/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.4634 - mae: 0.9350 - mse: 4.4634\n",
            "Epoch 18/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.4638 - mae: 0.9351 - mse: 4.4638\n",
            "Epoch 19/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.4647 - mae: 0.9350 - mse: 4.4647\n",
            "Epoch 20/100\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.4643 - mae: 0.9354 - mse: 4.4643\n",
            "Epoch 21/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.4639 - mae: 0.9352 - mse: 4.4639\n",
            "Epoch 22/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.4644 - mae: 0.9352 - mse: 4.4644\n",
            "Epoch 23/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.4645 - mae: 0.9355 - mse: 4.4645\n",
            "Epoch 24/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.4635 - mae: 0.9351 - mse: 4.4635\n",
            "Epoch 25/100\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.4632 - mae: 0.9351 - mse: 4.4632\n",
            "Epoch 26/100\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.4646 - mae: 0.9352 - mse: 4.4646\n",
            "Epoch 27/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.4644 - mae: 0.9351 - mse: 4.4644\n",
            "Epoch 28/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.4650 - mae: 0.9348 - mse: 4.4650\n",
            "Epoch 29/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.4639 - mae: 0.9352 - mse: 4.4639\n",
            "Epoch 30/100\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.4655 - mae: 0.9350 - mse: 4.4655\n",
            "Epoch 31/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.4637 - mae: 0.9352 - mse: 4.4637\n",
            "Epoch 32/100\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.4639 - mae: 0.9352 - mse: 4.4639\n",
            "Epoch 33/100\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.4634 - mae: 0.9351 - mse: 4.4634\n",
            "Epoch 34/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.4632 - mae: 0.9353 - mse: 4.4632\n",
            "Epoch 35/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.4636 - mae: 0.9350 - mse: 4.4636\n",
            "Epoch 36/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.4639 - mae: 0.9352 - mse: 4.4639\n",
            "Epoch 37/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.4641 - mae: 0.9352 - mse: 4.4641\n",
            "Epoch 38/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.4636 - mae: 0.9348 - mse: 4.4636\n",
            "Epoch 39/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.4636 - mae: 0.9350 - mse: 4.4636\n",
            "Epoch 40/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.4637 - mae: 0.9350 - mse: 4.4637\n",
            "Epoch 41/100\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.4627 - mae: 0.9353 - mse: 4.4627\n",
            "Epoch 42/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.4635 - mae: 0.9353 - mse: 4.4635\n",
            "Epoch 43/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.4650 - mae: 0.9354 - mse: 4.4650\n",
            "Epoch 44/100\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.4645 - mae: 0.9347 - mse: 4.4645\n",
            "Epoch 45/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.4642 - mae: 0.9352 - mse: 4.4642\n",
            "Epoch 46/100\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.4637 - mae: 0.9353 - mse: 4.4637\n",
            "Epoch 47/100\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.4640 - mae: 0.9353 - mse: 4.4640\n",
            "Epoch 48/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.4638 - mae: 0.9348 - mse: 4.4638\n",
            "Epoch 49/100\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.4643 - mae: 0.9353 - mse: 4.4643\n",
            "Epoch 50/100\n",
            "10093/10093 [==============================] - 20s 2ms/step - loss: 4.4631 - mae: 0.9349 - mse: 4.4631\n",
            "Epoch 51/100\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.4630 - mae: 0.9352 - mse: 4.4630\n",
            "Epoch 52/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.4643 - mae: 0.9355 - mse: 4.4643\n",
            "Epoch 53/100\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.4639 - mae: 0.9351 - mse: 4.4639\n",
            "Epoch 54/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.4632 - mae: 0.9354 - mse: 4.4632\n",
            "Epoch 55/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.4635 - mae: 0.9352 - mse: 4.4635\n",
            "Epoch 56/100\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.4638 - mae: 0.9353 - mse: 4.4638\n",
            "Epoch 57/100\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.4636 - mae: 0.9352 - mse: 4.4636\n",
            "Epoch 58/100\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.4642 - mae: 0.9348 - mse: 4.4642\n",
            "Epoch 59/100\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.4643 - mae: 0.9355 - mse: 4.4643\n",
            "Epoch 60/100\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.4648 - mae: 0.9352 - mse: 4.4648\n",
            "Epoch 61/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.4632 - mae: 0.9353 - mse: 4.4632\n",
            "Epoch 62/100\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.4644 - mae: 0.9354 - mse: 4.4644\n",
            "Epoch 63/100\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.4627 - mae: 0.9353 - mse: 4.4627\n",
            "Epoch 64/100\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.4636 - mae: 0.9353 - mse: 4.4636\n",
            "Epoch 65/100\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.4641 - mae: 0.9351 - mse: 4.4641\n",
            "Epoch 66/100\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.4643 - mae: 0.9356 - mse: 4.4643\n",
            "Epoch 67/100\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.4634 - mae: 0.9355 - mse: 4.4634\n",
            "Epoch 68/100\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.4636 - mae: 0.9349 - mse: 4.4636\n",
            "Epoch 69/100\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.4624 - mae: 0.9352 - mse: 4.4624\n",
            "Epoch 70/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.4644 - mae: 0.9356 - mse: 4.4644\n",
            "Epoch 71/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.4627 - mae: 0.9353 - mse: 4.4627\n",
            "Epoch 72/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.4633 - mae: 0.9353 - mse: 4.4633\n",
            "Epoch 73/100\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.4647 - mae: 0.9350 - mse: 4.4647\n",
            "Epoch 74/100\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.4643 - mae: 0.9352 - mse: 4.4643\n",
            "Epoch 75/100\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.4644 - mae: 0.9351 - mse: 4.4644\n",
            "Epoch 76/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.4641 - mae: 0.9349 - mse: 4.4641\n",
            "Epoch 77/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.4631 - mae: 0.9358 - mse: 4.4631\n",
            "Epoch 78/100\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.4642 - mae: 0.9351 - mse: 4.4642\n",
            "Epoch 79/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.4634 - mae: 0.9352 - mse: 4.4634\n",
            "Epoch 80/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.4635 - mae: 0.9355 - mse: 4.4635\n",
            "Epoch 81/100\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.4642 - mae: 0.9351 - mse: 4.4642\n",
            "Epoch 82/100\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.4643 - mae: 0.9348 - mse: 4.4643\n",
            "Epoch 83/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.4632 - mae: 0.9355 - mse: 4.4632\n",
            "Epoch 84/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.4644 - mae: 0.9351 - mse: 4.4644\n",
            "Epoch 85/100\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.4639 - mae: 0.9354 - mse: 4.4639\n",
            "Epoch 86/100\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.4630 - mae: 0.9353 - mse: 4.4630\n",
            "Epoch 87/100\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.4636 - mae: 0.9352 - mse: 4.4636\n",
            "Epoch 88/100\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.4645 - mae: 0.9351 - mse: 4.4645\n",
            "Epoch 89/100\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.4626 - mae: 0.9352 - mse: 4.4626\n",
            "Epoch 90/100\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.4640 - mae: 0.9351 - mse: 4.4640\n",
            "Epoch 91/100\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.4650 - mae: 0.9354 - mse: 4.4650\n",
            "Epoch 92/100\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.4644 - mae: 0.9357 - mse: 4.4644\n",
            "Epoch 93/100\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.4643 - mae: 0.9352 - mse: 4.4643\n",
            "Epoch 94/100\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.4638 - mae: 0.9348 - mse: 4.4638\n",
            "Epoch 95/100\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.4624 - mae: 0.9351 - mse: 4.4624\n",
            "Epoch 96/100\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.4629 - mae: 0.9354 - mse: 4.4629\n",
            "Epoch 97/100\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.4635 - mae: 0.9349 - mse: 4.4635\n",
            "Epoch 98/100\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.4656 - mae: 0.9354 - mse: 4.4656\n",
            "Epoch 99/100\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.4636 - mae: 0.9353 - mse: 4.4636\n",
            "Epoch 100/100\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.4642 - mae: 0.9352 - mse: 4.4642\n",
            "Epoch 1/3\n",
            "10093/10093 [==============================] - 20s 2ms/step - loss: 4.4638 - mae: 0.9353 - mse: 4.4638\n",
            "Epoch 2/3\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.4628 - mae: 0.9353 - mse: 4.4628\n",
            "Epoch 3/3\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.4637 - mae: 0.9353 - mse: 4.4637\n",
            "16221/16221 [==============================] - 23s 1ms/step - loss: 4.5802 - mae: 0.9356 - mse: 4.5802\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4.580198287963867, 0.935560941696167, 4.580198287963867]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehMZ2FZbSQYT"
      },
      "source": [
        "####7.4 modelo com 4 camada"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGsHHQdiTMzh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be5e557d-fe27-488c-b418-1a1fa7652d98"
      },
      "source": [
        "modelo_4C = denseLayer4()\n",
        "best_epoch = modelo_4C.fit(pr_treino,sal_treino,epochs=100,batch_size=120)\n",
        "val_acc_per_epoch = best_epoch.history['mae']\n",
        "best_epoch = val_acc_per_epoch.index(min(val_acc_per_epoch))+1\n",
        "modelo_4C.fit(pr_treino,sal_treino,epochs=best_epoch,batch_size=120)\n",
        "modelo_4C.evaluate(pr_teste,sal_teste)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 31.6251 - mae: 3.9943 - mse: 31.6251\n",
            "Epoch 2/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 12.0982 - mae: 2.4700 - mse: 12.0982\n",
            "Epoch 3/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 5.7864 - mae: 1.3764 - mse: 5.7864\n",
            "Epoch 4/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.4734 - mae: 0.9608 - mse: 4.4734\n",
            "Epoch 5/100\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.3963 - mae: 0.9146 - mse: 4.3963\n",
            "Epoch 6/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.3903 - mae: 0.9119 - mse: 4.3903\n",
            "Epoch 7/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.3890 - mae: 0.9110 - mse: 4.3890\n",
            "Epoch 8/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.3868 - mae: 0.9111 - mse: 4.3868\n",
            "Epoch 9/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.3850 - mae: 0.9102 - mse: 4.3850\n",
            "Epoch 10/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.3768 - mae: 0.9078 - mse: 4.3768\n",
            "Epoch 11/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.3298 - mae: 0.8977 - mse: 4.3298\n",
            "Epoch 12/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.3027 - mae: 0.8871 - mse: 4.3027\n",
            "Epoch 13/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2928 - mae: 0.8837 - mse: 4.2928\n",
            "Epoch 14/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2872 - mae: 0.8828 - mse: 4.2872\n",
            "Epoch 15/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2845 - mae: 0.8814 - mse: 4.2845\n",
            "Epoch 16/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2733 - mae: 0.8776 - mse: 4.2733\n",
            "Epoch 17/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2630 - mae: 0.8727 - mse: 4.2630\n",
            "Epoch 18/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2579 - mae: 0.8715 - mse: 4.2579\n",
            "Epoch 19/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2547 - mae: 0.8708 - mse: 4.2547\n",
            "Epoch 20/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2509 - mae: 0.8700 - mse: 4.2509\n",
            "Epoch 21/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2496 - mae: 0.8713 - mse: 4.2496\n",
            "Epoch 22/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2464 - mae: 0.8703 - mse: 4.2464\n",
            "Epoch 23/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2439 - mae: 0.8699 - mse: 4.2439\n",
            "Epoch 24/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2447 - mae: 0.8697 - mse: 4.2447\n",
            "Epoch 25/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2451 - mae: 0.8707 - mse: 4.2451\n",
            "Epoch 26/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2426 - mae: 0.8697 - mse: 4.2426\n",
            "Epoch 27/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2407 - mae: 0.8688 - mse: 4.2407\n",
            "Epoch 28/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2394 - mae: 0.8694 - mse: 4.2394\n",
            "Epoch 29/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2385 - mae: 0.8697 - mse: 4.2385\n",
            "Epoch 30/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2388 - mae: 0.8684 - mse: 4.2388\n",
            "Epoch 31/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2346 - mae: 0.8690 - mse: 4.2346\n",
            "Epoch 32/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2310 - mae: 0.8689 - mse: 4.2310\n",
            "Epoch 33/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2252 - mae: 0.8680 - mse: 4.2252\n",
            "Epoch 34/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2252 - mae: 0.8673 - mse: 4.2252\n",
            "Epoch 35/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2264 - mae: 0.8681 - mse: 4.2264\n",
            "Epoch 36/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2222 - mae: 0.8673 - mse: 4.2222\n",
            "Epoch 37/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2229 - mae: 0.8687 - mse: 4.2229\n",
            "Epoch 38/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2212 - mae: 0.8672 - mse: 4.2212\n",
            "Epoch 39/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2227 - mae: 0.8672 - mse: 4.2227\n",
            "Epoch 40/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2209 - mae: 0.8673 - mse: 4.2209\n",
            "Epoch 41/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2215 - mae: 0.8679 - mse: 4.2215\n",
            "Epoch 42/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2214 - mae: 0.8675 - mse: 4.2214\n",
            "Epoch 43/100\n",
            "10093/10093 [==============================] - 20s 2ms/step - loss: 4.2219 - mae: 0.8678 - mse: 4.2219\n",
            "Epoch 44/100\n",
            "10093/10093 [==============================] - 20s 2ms/step - loss: 4.2190 - mae: 0.8677 - mse: 4.2190\n",
            "Epoch 45/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2184 - mae: 0.8673 - mse: 4.2184\n",
            "Epoch 46/100\n",
            "10093/10093 [==============================] - 17s 2ms/step - loss: 4.2173 - mae: 0.8671 - mse: 4.2173\n",
            "Epoch 47/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2200 - mae: 0.8678 - mse: 4.2200\n",
            "Epoch 48/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2139 - mae: 0.8671 - mse: 4.2139\n",
            "Epoch 49/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2161 - mae: 0.8673 - mse: 4.2161\n",
            "Epoch 50/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2150 - mae: 0.8676 - mse: 4.2150\n",
            "Epoch 51/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2164 - mae: 0.8671 - mse: 4.2164\n",
            "Epoch 52/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2156 - mae: 0.8672 - mse: 4.2156\n",
            "Epoch 53/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2139 - mae: 0.8663 - mse: 4.2139\n",
            "Epoch 54/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2170 - mae: 0.8675 - mse: 4.2170\n",
            "Epoch 55/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2137 - mae: 0.8666 - mse: 4.2137\n",
            "Epoch 56/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2153 - mae: 0.8670 - mse: 4.2153\n",
            "Epoch 57/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2135 - mae: 0.8664 - mse: 4.2135\n",
            "Epoch 58/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2103 - mae: 0.8666 - mse: 4.2103\n",
            "Epoch 59/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2092 - mae: 0.8663 - mse: 4.2092\n",
            "Epoch 60/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2102 - mae: 0.8657 - mse: 4.2102\n",
            "Epoch 61/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2118 - mae: 0.8668 - mse: 4.2118\n",
            "Epoch 62/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2122 - mae: 0.8666 - mse: 4.2122\n",
            "Epoch 63/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2110 - mae: 0.8665 - mse: 4.2110\n",
            "Epoch 64/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2110 - mae: 0.8664 - mse: 4.2110\n",
            "Epoch 65/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2142 - mae: 0.8675 - mse: 4.2142\n",
            "Epoch 66/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2127 - mae: 0.8659 - mse: 4.2127\n",
            "Epoch 67/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2106 - mae: 0.8667 - mse: 4.2106\n",
            "Epoch 68/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2104 - mae: 0.8661 - mse: 4.2104\n",
            "Epoch 69/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2087 - mae: 0.8654 - mse: 4.2087\n",
            "Epoch 70/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2068 - mae: 0.8645 - mse: 4.2068\n",
            "Epoch 71/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2102 - mae: 0.8655 - mse: 4.2102\n",
            "Epoch 72/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2088 - mae: 0.8655 - mse: 4.2088\n",
            "Epoch 73/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2092 - mae: 0.8661 - mse: 4.2092\n",
            "Epoch 74/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2070 - mae: 0.8656 - mse: 4.2070\n",
            "Epoch 75/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2080 - mae: 0.8649 - mse: 4.2080\n",
            "Epoch 76/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2061 - mae: 0.8649 - mse: 4.2061\n",
            "Epoch 77/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2094 - mae: 0.8656 - mse: 4.2094\n",
            "Epoch 78/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2092 - mae: 0.8653 - mse: 4.2092\n",
            "Epoch 79/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2083 - mae: 0.8654 - mse: 4.2083\n",
            "Epoch 80/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2070 - mae: 0.8649 - mse: 4.2070\n",
            "Epoch 81/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2069 - mae: 0.8647 - mse: 4.2069\n",
            "Epoch 82/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2059 - mae: 0.8654 - mse: 4.2059\n",
            "Epoch 83/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2048 - mae: 0.8645 - mse: 4.2048\n",
            "Epoch 84/100\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.2073 - mae: 0.8651 - mse: 4.2073\n",
            "Epoch 85/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2035 - mae: 0.8649 - mse: 4.2035\n",
            "Epoch 86/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2062 - mae: 0.8652 - mse: 4.2062\n",
            "Epoch 87/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2039 - mae: 0.8653 - mse: 4.2039\n",
            "Epoch 88/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2073 - mae: 0.8656 - mse: 4.2073\n",
            "Epoch 89/100\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.2055 - mae: 0.8648 - mse: 4.2055\n",
            "Epoch 90/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2051 - mae: 0.8646 - mse: 4.2051\n",
            "Epoch 91/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2031 - mae: 0.8642 - mse: 4.2031\n",
            "Epoch 92/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2025 - mae: 0.8644 - mse: 4.2025\n",
            "Epoch 93/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2049 - mae: 0.8652 - mse: 4.2049\n",
            "Epoch 94/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2038 - mae: 0.8642 - mse: 4.2038\n",
            "Epoch 95/100\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.2087 - mae: 0.8659 - mse: 4.2087\n",
            "Epoch 96/100\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.2003 - mae: 0.8645 - mse: 4.2003\n",
            "Epoch 97/100\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.2046 - mae: 0.8652 - mse: 4.2046\n",
            "Epoch 98/100\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.2054 - mae: 0.8643 - mse: 4.2054\n",
            "Epoch 99/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2032 - mae: 0.8644 - mse: 4.2032\n",
            "Epoch 100/100\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2028 - mae: 0.8642 - mse: 4.2028\n",
            "Epoch 1/94\n",
            "10093/10093 [==============================] - 20s 2ms/step - loss: 4.2027 - mae: 0.8645 - mse: 4.2027\n",
            "Epoch 2/94\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.1995 - mae: 0.8640 - mse: 4.1995\n",
            "Epoch 3/94\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2048 - mae: 0.8647 - mse: 4.2048\n",
            "Epoch 4/94\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.1998 - mae: 0.8641 - mse: 4.1998\n",
            "Epoch 5/94\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.2032 - mae: 0.8642 - mse: 4.2032\n",
            "Epoch 6/94\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2022 - mae: 0.8643 - mse: 4.2022\n",
            "Epoch 7/94\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.1996 - mae: 0.8641 - mse: 4.1996\n",
            "Epoch 8/94\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.1979 - mae: 0.8638 - mse: 4.1979\n",
            "Epoch 9/94\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.1956 - mae: 0.8640 - mse: 4.1956\n",
            "Epoch 10/94\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.1981 - mae: 0.8639 - mse: 4.1981\n",
            "Epoch 11/94\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.2011 - mae: 0.8646 - mse: 4.2011\n",
            "Epoch 12/94\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.1986 - mae: 0.8640 - mse: 4.1986\n",
            "Epoch 13/94\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.1996 - mae: 0.8646 - mse: 4.1996\n",
            "Epoch 14/94\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.2007 - mae: 0.8650 - mse: 4.2007\n",
            "Epoch 15/94\n",
            "10093/10093 [==============================] - 20s 2ms/step - loss: 4.1984 - mae: 0.8638 - mse: 4.1984\n",
            "Epoch 16/94\n",
            "10093/10093 [==============================] - 20s 2ms/step - loss: 4.1983 - mae: 0.8647 - mse: 4.1983\n",
            "Epoch 17/94\n",
            "10093/10093 [==============================] - 20s 2ms/step - loss: 4.1988 - mae: 0.8634 - mse: 4.1988\n",
            "Epoch 18/94\n",
            "10093/10093 [==============================] - 20s 2ms/step - loss: 4.2015 - mae: 0.8649 - mse: 4.2015\n",
            "Epoch 19/94\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.2005 - mae: 0.8647 - mse: 4.2005\n",
            "Epoch 20/94\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.1989 - mae: 0.8638 - mse: 4.1989\n",
            "Epoch 21/94\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.1999 - mae: 0.8644 - mse: 4.1999\n",
            "Epoch 22/94\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.2001 - mae: 0.8638 - mse: 4.2001\n",
            "Epoch 23/94\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.1985 - mae: 0.8634 - mse: 4.1985\n",
            "Epoch 24/94\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.1991 - mae: 0.8636 - mse: 4.1991\n",
            "Epoch 25/94\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.2007 - mae: 0.8637 - mse: 4.2007\n",
            "Epoch 26/94\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.1966 - mae: 0.8632 - mse: 4.1966\n",
            "Epoch 27/94\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.1985 - mae: 0.8641 - mse: 4.1985\n",
            "Epoch 28/94\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.1977 - mae: 0.8634 - mse: 4.1977\n",
            "Epoch 29/94\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.1959 - mae: 0.8633 - mse: 4.1959\n",
            "Epoch 30/94\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.1956 - mae: 0.8634 - mse: 4.1956\n",
            "Epoch 31/94\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.1981 - mae: 0.8629 - mse: 4.1981\n",
            "Epoch 32/94\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.1969 - mae: 0.8627 - mse: 4.1969\n",
            "Epoch 33/94\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.1987 - mae: 0.8635 - mse: 4.1987\n",
            "Epoch 34/94\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.1985 - mae: 0.8633 - mse: 4.1985\n",
            "Epoch 35/94\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.2004 - mae: 0.8638 - mse: 4.2004\n",
            "Epoch 36/94\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.1988 - mae: 0.8635 - mse: 4.1988\n",
            "Epoch 37/94\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.1982 - mae: 0.8641 - mse: 4.1982\n",
            "Epoch 38/94\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.1980 - mae: 0.8631 - mse: 4.1980\n",
            "Epoch 39/94\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.1982 - mae: 0.8631 - mse: 4.1982\n",
            "Epoch 40/94\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.1982 - mae: 0.8635 - mse: 4.1982\n",
            "Epoch 41/94\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.1985 - mae: 0.8630 - mse: 4.1985\n",
            "Epoch 42/94\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.1969 - mae: 0.8623 - mse: 4.1969\n",
            "Epoch 43/94\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.1980 - mae: 0.8631 - mse: 4.1980\n",
            "Epoch 44/94\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.1962 - mae: 0.8631 - mse: 4.1962\n",
            "Epoch 45/94\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.1964 - mae: 0.8634 - mse: 4.1964\n",
            "Epoch 46/94\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.1979 - mae: 0.8631 - mse: 4.1979\n",
            "Epoch 47/94\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.1957 - mae: 0.8633 - mse: 4.1957\n",
            "Epoch 48/94\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.1966 - mae: 0.8627 - mse: 4.1966\n",
            "Epoch 49/94\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.1925 - mae: 0.8634 - mse: 4.1925\n",
            "Epoch 50/94\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.1985 - mae: 0.8642 - mse: 4.1985\n",
            "Epoch 51/94\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.1939 - mae: 0.8631 - mse: 4.1939\n",
            "Epoch 52/94\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.1966 - mae: 0.8634 - mse: 4.1966\n",
            "Epoch 53/94\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.1948 - mae: 0.8635 - mse: 4.1948\n",
            "Epoch 54/94\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.1947 - mae: 0.8628 - mse: 4.1947\n",
            "Epoch 55/94\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.1963 - mae: 0.8633 - mse: 4.1963\n",
            "Epoch 56/94\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.1946 - mae: 0.8633 - mse: 4.1946\n",
            "Epoch 57/94\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.1926 - mae: 0.8623 - mse: 4.1926\n",
            "Epoch 58/94\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.1954 - mae: 0.8635 - mse: 4.1954\n",
            "Epoch 59/94\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.1961 - mae: 0.8627 - mse: 4.1961\n",
            "Epoch 60/94\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.1923 - mae: 0.8627 - mse: 4.1923\n",
            "Epoch 61/94\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.1938 - mae: 0.8629 - mse: 4.1938\n",
            "Epoch 62/94\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.1921 - mae: 0.8629 - mse: 4.1921\n",
            "Epoch 63/94\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.1949 - mae: 0.8631 - mse: 4.1949\n",
            "Epoch 64/94\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.1911 - mae: 0.8628 - mse: 4.1911\n",
            "Epoch 65/94\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.1937 - mae: 0.8632 - mse: 4.1937\n",
            "Epoch 66/94\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.1933 - mae: 0.8626 - mse: 4.1933\n",
            "Epoch 67/94\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.1950 - mae: 0.8627 - mse: 4.1950\n",
            "Epoch 68/94\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.1959 - mae: 0.8631 - mse: 4.1959\n",
            "Epoch 69/94\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.1924 - mae: 0.8629 - mse: 4.1924\n",
            "Epoch 70/94\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.1949 - mae: 0.8633 - mse: 4.1949\n",
            "Epoch 71/94\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.1937 - mae: 0.8630 - mse: 4.1937\n",
            "Epoch 72/94\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.1922 - mae: 0.8624 - mse: 4.1922\n",
            "Epoch 73/94\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.1941 - mae: 0.8630 - mse: 4.1941\n",
            "Epoch 74/94\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.1936 - mae: 0.8630 - mse: 4.1936\n",
            "Epoch 75/94\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.1916 - mae: 0.8621 - mse: 4.1916\n",
            "Epoch 76/94\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.1904 - mae: 0.8628 - mse: 4.1904\n",
            "Epoch 77/94\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.1936 - mae: 0.8625 - mse: 4.1936\n",
            "Epoch 78/94\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.1913 - mae: 0.8629 - mse: 4.1913\n",
            "Epoch 79/94\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.1941 - mae: 0.8629 - mse: 4.1941\n",
            "Epoch 80/94\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.1908 - mae: 0.8630 - mse: 4.1908\n",
            "Epoch 81/94\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.1928 - mae: 0.8627 - mse: 4.1928\n",
            "Epoch 82/94\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.1946 - mae: 0.8630 - mse: 4.1946\n",
            "Epoch 83/94\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.1927 - mae: 0.8625 - mse: 4.1927\n",
            "Epoch 84/94\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.1913 - mae: 0.8628 - mse: 4.1913\n",
            "Epoch 85/94\n",
            "10093/10093 [==============================] - 20s 2ms/step - loss: 4.1913 - mae: 0.8628 - mse: 4.1913\n",
            "Epoch 86/94\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.1921 - mae: 0.8633 - mse: 4.1921\n",
            "Epoch 87/94\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.1923 - mae: 0.8623 - mse: 4.1923\n",
            "Epoch 88/94\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.1926 - mae: 0.8632 - mse: 4.1926\n",
            "Epoch 89/94\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.1938 - mae: 0.8632 - mse: 4.1938\n",
            "Epoch 90/94\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.1885 - mae: 0.8622 - mse: 4.1885\n",
            "Epoch 91/94\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.1896 - mae: 0.8625 - mse: 4.1896\n",
            "Epoch 92/94\n",
            "10093/10093 [==============================] - 18s 2ms/step - loss: 4.1909 - mae: 0.8626 - mse: 4.1909\n",
            "Epoch 93/94\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.1930 - mae: 0.8630 - mse: 4.1930\n",
            "Epoch 94/94\n",
            "10093/10093 [==============================] - 19s 2ms/step - loss: 4.1919 - mae: 0.8635 - mse: 4.1919\n",
            "16221/16221 [==============================] - 23s 1ms/step - loss: 4.3057 - mae: 0.8898 - mse: 4.3057\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4.30572509765625, 0.8898487687110901, 4.30572509765625]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxkF9ROXSQgs"
      },
      "source": [
        "####7.5 modelo com 5 camada"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knSaFZKkTQPy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13f7100a-d83a-4af8-a7dd-63d1161ac8cf"
      },
      "source": [
        "modelo_5C = denseLayer5()\n",
        "best_epoch = modelo_5C.fit(pr_treino,sal_treino,epochs=100,batch_size=120)\n",
        "val_acc_per_epoch = best_epoch.history['mae']\n",
        "best_epoch = val_acc_per_epoch.index(min(val_acc_per_epoch))+1\n",
        "modelo_5C.fit(pr_treino,sal_treino,epochs=best_epoch,batch_size=120)\n",
        "modelo_5C.evaluate(pr_teste,sal_teste)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 42.7461 - mae: 4.5057 - mse: 42.7461\n",
            "Epoch 2/100\n",
            "10093/10093 [==============================] - 20s 2ms/step - loss: 4.4756 - mae: 0.9388 - mse: 4.4756\n",
            "Epoch 3/100\n",
            "10093/10093 [==============================] - 20s 2ms/step - loss: 4.4472 - mae: 0.9284 - mse: 4.4472\n",
            "Epoch 4/100\n",
            "10093/10093 [==============================] - 20s 2ms/step - loss: 4.4144 - mae: 0.9180 - mse: 4.4144\n",
            "Epoch 5/100\n",
            "10093/10093 [==============================] - 20s 2ms/step - loss: 4.4107 - mae: 0.9174 - mse: 4.4107\n",
            "Epoch 6/100\n",
            "10093/10093 [==============================] - 20s 2ms/step - loss: 4.4102 - mae: 0.9173 - mse: 4.4102\n",
            "Epoch 7/100\n",
            "10093/10093 [==============================] - 20s 2ms/step - loss: 4.4078 - mae: 0.9172 - mse: 4.4078\n",
            "Epoch 8/100\n",
            "10093/10093 [==============================] - 20s 2ms/step - loss: 4.4079 - mae: 0.9167 - mse: 4.4079\n",
            "Epoch 9/100\n",
            "10093/10093 [==============================] - 20s 2ms/step - loss: 4.4081 - mae: 0.9169 - mse: 4.4081\n",
            "Epoch 10/100\n",
            "10093/10093 [==============================] - 20s 2ms/step - loss: 4.4076 - mae: 0.9166 - mse: 4.4076\n",
            "Epoch 11/100\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.4092 - mae: 0.9167 - mse: 4.4092\n",
            "Epoch 12/100\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.4154 - mae: 0.9191 - mse: 4.4154\n",
            "Epoch 13/100\n",
            "10093/10093 [==============================] - 20s 2ms/step - loss: 4.4135 - mae: 0.9194 - mse: 4.4135\n",
            "Epoch 14/100\n",
            "10093/10093 [==============================] - 20s 2ms/step - loss: 4.4118 - mae: 0.9189 - mse: 4.4118\n",
            "Epoch 15/100\n",
            "10093/10093 [==============================] - 20s 2ms/step - loss: 4.4117 - mae: 0.9189 - mse: 4.4117\n",
            "Epoch 16/100\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.4113 - mae: 0.9186 - mse: 4.4113\n",
            "Epoch 17/100\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.4078 - mae: 0.9183 - mse: 4.4078\n",
            "Epoch 18/100\n",
            "10093/10093 [==============================] - 20s 2ms/step - loss: 4.4097 - mae: 0.9182 - mse: 4.4097\n",
            "Epoch 19/100\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.4039 - mae: 0.9175 - mse: 4.4039\n",
            "Epoch 20/100\n",
            "10093/10093 [==============================] - 20s 2ms/step - loss: 4.4037 - mae: 0.9175 - mse: 4.4037\n",
            "Epoch 21/100\n",
            "10093/10093 [==============================] - 20s 2ms/step - loss: 4.4028 - mae: 0.9169 - mse: 4.4028\n",
            "Epoch 22/100\n",
            "10093/10093 [==============================] - 20s 2ms/step - loss: 4.4028 - mae: 0.9168 - mse: 4.4028\n",
            "Epoch 23/100\n",
            "10093/10093 [==============================] - 20s 2ms/step - loss: 4.4015 - mae: 0.9165 - mse: 4.4015\n",
            "Epoch 24/100\n",
            "10093/10093 [==============================] - 20s 2ms/step - loss: 4.4010 - mae: 0.9159 - mse: 4.4010\n",
            "Epoch 25/100\n",
            "10093/10093 [==============================] - 20s 2ms/step - loss: 4.3964 - mae: 0.9150 - mse: 4.3964\n",
            "Epoch 26/100\n",
            "10093/10093 [==============================] - 20s 2ms/step - loss: 4.4007 - mae: 0.9157 - mse: 4.4007\n",
            "Epoch 27/100\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.4135 - mae: 0.9185 - mse: 4.4135\n",
            "Epoch 28/100\n",
            "10093/10093 [==============================] - 20s 2ms/step - loss: 4.4048 - mae: 0.9173 - mse: 4.4048\n",
            "Epoch 29/100\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.4075 - mae: 0.9161 - mse: 4.4075\n",
            "Epoch 30/100\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.4116 - mae: 0.9178 - mse: 4.4116\n",
            "Epoch 31/100\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.4044 - mae: 0.9160 - mse: 4.4044\n",
            "Epoch 32/100\n",
            "10093/10093 [==============================] - 20s 2ms/step - loss: 4.3933 - mae: 0.9130 - mse: 4.3933\n",
            "Epoch 33/100\n",
            "10093/10093 [==============================] - 20s 2ms/step - loss: 4.3926 - mae: 0.9120 - mse: 4.3926\n",
            "Epoch 34/100\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.3986 - mae: 0.9141 - mse: 4.3986\n",
            "Epoch 35/100\n",
            "10093/10093 [==============================] - 20s 2ms/step - loss: 4.4017 - mae: 0.9149 - mse: 4.4017\n",
            "Epoch 36/100\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.3990 - mae: 0.9147 - mse: 4.3990\n",
            "Epoch 37/100\n",
            "10093/10093 [==============================] - 23s 2ms/step - loss: 4.4035 - mae: 0.9157 - mse: 4.4035\n",
            "Epoch 38/100\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.3952 - mae: 0.9128 - mse: 4.3952\n",
            "Epoch 39/100\n",
            "10093/10093 [==============================] - 20s 2ms/step - loss: 4.3939 - mae: 0.9126 - mse: 4.3939\n",
            "Epoch 40/100\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.3947 - mae: 0.9125 - mse: 4.3947\n",
            "Epoch 41/100\n",
            "10093/10093 [==============================] - 22s 2ms/step - loss: 4.3891 - mae: 0.9110 - mse: 4.3891\n",
            "Epoch 42/100\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.3913 - mae: 0.9116 - mse: 4.3913\n",
            "Epoch 43/100\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.3921 - mae: 0.9124 - mse: 4.3921\n",
            "Epoch 44/100\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.3958 - mae: 0.9130 - mse: 4.3958\n",
            "Epoch 45/100\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.4002 - mae: 0.9136 - mse: 4.4002\n",
            "Epoch 46/100\n",
            "10093/10093 [==============================] - 20s 2ms/step - loss: 4.3903 - mae: 0.9118 - mse: 4.3903\n",
            "Epoch 47/100\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.3898 - mae: 0.9115 - mse: 4.3898\n",
            "Epoch 48/100\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.3873 - mae: 0.9111 - mse: 4.3873\n",
            "Epoch 49/100\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.3878 - mae: 0.9114 - mse: 4.3878\n",
            "Epoch 50/100\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.3888 - mae: 0.9113 - mse: 4.3888\n",
            "Epoch 51/100\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.3924 - mae: 0.9127 - mse: 4.3924\n",
            "Epoch 52/100\n",
            "10093/10093 [==============================] - 20s 2ms/step - loss: 4.3912 - mae: 0.9123 - mse: 4.3912\n",
            "Epoch 53/100\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.3887 - mae: 0.9115 - mse: 4.3887\n",
            "Epoch 54/100\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.3844 - mae: 0.9102 - mse: 4.3844\n",
            "Epoch 55/100\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.3868 - mae: 0.9110 - mse: 4.3868\n",
            "Epoch 56/100\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.3891 - mae: 0.9109 - mse: 4.3891\n",
            "Epoch 57/100\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.3832 - mae: 0.9097 - mse: 4.3832\n",
            "Epoch 58/100\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.3854 - mae: 0.9097 - mse: 4.3854\n",
            "Epoch 59/100\n",
            "10093/10093 [==============================] - 22s 2ms/step - loss: 4.4033 - mae: 0.9148 - mse: 4.4033\n",
            "Epoch 60/100\n",
            "10093/10093 [==============================] - 22s 2ms/step - loss: 4.4038 - mae: 0.9159 - mse: 4.4038\n",
            "Epoch 61/100\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.3865 - mae: 0.9109 - mse: 4.3865\n",
            "Epoch 62/100\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.3877 - mae: 0.9123 - mse: 4.3877\n",
            "Epoch 63/100\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.4113 - mae: 0.9180 - mse: 4.4113\n",
            "Epoch 64/100\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.4118 - mae: 0.9183 - mse: 4.4118\n",
            "Epoch 65/100\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.4121 - mae: 0.9176 - mse: 4.4121\n",
            "Epoch 66/100\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.4109 - mae: 0.9175 - mse: 4.4109\n",
            "Epoch 67/100\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.4142 - mae: 0.9176 - mse: 4.4142\n",
            "Epoch 68/100\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.4106 - mae: 0.9165 - mse: 4.4106\n",
            "Epoch 69/100\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.4735 - mae: 0.9262 - mse: 4.4735\n",
            "Epoch 70/100\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.4036 - mae: 0.9149 - mse: 4.4036\n",
            "Epoch 71/100\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.3968 - mae: 0.9139 - mse: 4.3968\n",
            "Epoch 72/100\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.3918 - mae: 0.9128 - mse: 4.3918\n",
            "Epoch 73/100\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.3867 - mae: 0.9124 - mse: 4.3867\n",
            "Epoch 74/100\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.3894 - mae: 0.9130 - mse: 4.3894\n",
            "Epoch 75/100\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.3873 - mae: 0.9130 - mse: 4.3873\n",
            "Epoch 76/100\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.3845 - mae: 0.9124 - mse: 4.3845\n",
            "Epoch 77/100\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.3808 - mae: 0.9120 - mse: 4.3808\n",
            "Epoch 78/100\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.3785 - mae: 0.9113 - mse: 4.3785\n",
            "Epoch 79/100\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.3751 - mae: 0.9102 - mse: 4.3751\n",
            "Epoch 80/100\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.3793 - mae: 0.9105 - mse: 4.3793\n",
            "Epoch 81/100\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.3804 - mae: 0.9099 - mse: 4.3804\n",
            "Epoch 82/100\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.3763 - mae: 0.9097 - mse: 4.3763\n",
            "Epoch 83/100\n",
            "10093/10093 [==============================] - 22s 2ms/step - loss: 4.3754 - mae: 0.9097 - mse: 4.3754\n",
            "Epoch 84/100\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.3757 - mae: 0.9098 - mse: 4.3757\n",
            "Epoch 85/100\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.4097 - mae: 0.9172 - mse: 4.4097\n",
            "Epoch 86/100\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.3968 - mae: 0.9123 - mse: 4.3968\n",
            "Epoch 87/100\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.3848 - mae: 0.9096 - mse: 4.3848\n",
            "Epoch 88/100\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.3815 - mae: 0.9094 - mse: 4.3815\n",
            "Epoch 89/100\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.3751 - mae: 0.9086 - mse: 4.3751\n",
            "Epoch 90/100\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.3755 - mae: 0.9089 - mse: 4.3755\n",
            "Epoch 91/100\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.3754 - mae: 0.9093 - mse: 4.3754\n",
            "Epoch 92/100\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.3757 - mae: 0.9091 - mse: 4.3757\n",
            "Epoch 93/100\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.3709 - mae: 0.9089 - mse: 4.3709\n",
            "Epoch 94/100\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.3743 - mae: 0.9093 - mse: 4.3743\n",
            "Epoch 95/100\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.3738 - mae: 0.9091 - mse: 4.3738\n",
            "Epoch 96/100\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.3728 - mae: 0.9092 - mse: 4.3728\n",
            "Epoch 97/100\n",
            "10093/10093 [==============================] - 22s 2ms/step - loss: 4.3731 - mae: 0.9087 - mse: 4.3731\n",
            "Epoch 98/100\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.3716 - mae: 0.9087 - mse: 4.3716\n",
            "Epoch 99/100\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.3735 - mae: 0.9090 - mse: 4.3735\n",
            "Epoch 100/100\n",
            "10093/10093 [==============================] - 22s 2ms/step - loss: 4.3711 - mae: 0.9092 - mse: 4.3711\n",
            "Epoch 1/89\n",
            "10093/10093 [==============================] - 22s 2ms/step - loss: 4.3722 - mae: 0.9089 - mse: 4.3722\n",
            "Epoch 2/89\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.3720 - mae: 0.9094 - mse: 4.3720\n",
            "Epoch 3/89\n",
            "10093/10093 [==============================] - 22s 2ms/step - loss: 4.3710 - mae: 0.9091 - mse: 4.3710\n",
            "Epoch 4/89\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.3723 - mae: 0.9087 - mse: 4.3723\n",
            "Epoch 5/89\n",
            "10093/10093 [==============================] - 22s 2ms/step - loss: 4.3708 - mae: 0.9088 - mse: 4.3708\n",
            "Epoch 6/89\n",
            "10093/10093 [==============================] - 23s 2ms/step - loss: 4.3729 - mae: 0.9083 - mse: 4.3729\n",
            "Epoch 7/89\n",
            "10093/10093 [==============================] - 25s 2ms/step - loss: 4.3702 - mae: 0.9084 - mse: 4.3702\n",
            "Epoch 8/89\n",
            "10093/10093 [==============================] - 24s 2ms/step - loss: 4.3687 - mae: 0.9087 - mse: 4.3687\n",
            "Epoch 9/89\n",
            "10093/10093 [==============================] - 26s 3ms/step - loss: 4.3709 - mae: 0.9083 - mse: 4.3709\n",
            "Epoch 10/89\n",
            "10093/10093 [==============================] - 26s 3ms/step - loss: 4.3697 - mae: 0.9086 - mse: 4.3697\n",
            "Epoch 11/89\n",
            "10093/10093 [==============================] - 23s 2ms/step - loss: 4.3670 - mae: 0.9073 - mse: 4.3670\n",
            "Epoch 12/89\n",
            "10093/10093 [==============================] - 22s 2ms/step - loss: 4.3681 - mae: 0.9075 - mse: 4.3681\n",
            "Epoch 13/89\n",
            "10093/10093 [==============================] - 22s 2ms/step - loss: 4.3677 - mae: 0.9072 - mse: 4.3677\n",
            "Epoch 14/89\n",
            "10093/10093 [==============================] - 22s 2ms/step - loss: 4.3664 - mae: 0.9075 - mse: 4.3664\n",
            "Epoch 15/89\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.3648 - mae: 0.9071 - mse: 4.3648\n",
            "Epoch 16/89\n",
            "10093/10093 [==============================] - 22s 2ms/step - loss: 4.3625 - mae: 0.9063 - mse: 4.3625\n",
            "Epoch 17/89\n",
            "10093/10093 [==============================] - 22s 2ms/step - loss: 4.3648 - mae: 0.9071 - mse: 4.3648\n",
            "Epoch 18/89\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.3665 - mae: 0.9069 - mse: 4.3665\n",
            "Epoch 19/89\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.3708 - mae: 0.9078 - mse: 4.3708\n",
            "Epoch 20/89\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.3734 - mae: 0.9085 - mse: 4.3734\n",
            "Epoch 21/89\n",
            "10093/10093 [==============================] - 22s 2ms/step - loss: 4.3706 - mae: 0.9081 - mse: 4.3706\n",
            "Epoch 22/89\n",
            "10093/10093 [==============================] - 22s 2ms/step - loss: 4.3732 - mae: 0.9077 - mse: 4.3732\n",
            "Epoch 23/89\n",
            "10093/10093 [==============================] - 22s 2ms/step - loss: 4.3732 - mae: 0.9079 - mse: 4.3732\n",
            "Epoch 24/89\n",
            "10093/10093 [==============================] - 22s 2ms/step - loss: 4.3693 - mae: 0.9080 - mse: 4.3693\n",
            "Epoch 25/89\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.3697 - mae: 0.9078 - mse: 4.3697\n",
            "Epoch 26/89\n",
            "10093/10093 [==============================] - 22s 2ms/step - loss: 4.3701 - mae: 0.9082 - mse: 4.3701\n",
            "Epoch 27/89\n",
            "10093/10093 [==============================] - 22s 2ms/step - loss: 4.3710 - mae: 0.9080 - mse: 4.3710\n",
            "Epoch 28/89\n",
            "10093/10093 [==============================] - 22s 2ms/step - loss: 4.3728 - mae: 0.9080 - mse: 4.3728\n",
            "Epoch 29/89\n",
            "10093/10093 [==============================] - 22s 2ms/step - loss: 4.3720 - mae: 0.9081 - mse: 4.3720\n",
            "Epoch 30/89\n",
            "10093/10093 [==============================] - 22s 2ms/step - loss: 4.3699 - mae: 0.9079 - mse: 4.3699\n",
            "Epoch 31/89\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.3693 - mae: 0.9087 - mse: 4.3693\n",
            "Epoch 32/89\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.3698 - mae: 0.9084 - mse: 4.3698\n",
            "Epoch 33/89\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.3695 - mae: 0.9083 - mse: 4.3695\n",
            "Epoch 34/89\n",
            "10093/10093 [==============================] - 22s 2ms/step - loss: 4.3680 - mae: 0.9084 - mse: 4.3680\n",
            "Epoch 35/89\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.3692 - mae: 0.9080 - mse: 4.3692\n",
            "Epoch 36/89\n",
            "10093/10093 [==============================] - 23s 2ms/step - loss: 4.3695 - mae: 0.9078 - mse: 4.3695\n",
            "Epoch 37/89\n",
            "10093/10093 [==============================] - 22s 2ms/step - loss: 4.3698 - mae: 0.9083 - mse: 4.3698\n",
            "Epoch 38/89\n",
            "10093/10093 [==============================] - 22s 2ms/step - loss: 4.3670 - mae: 0.9082 - mse: 4.3670\n",
            "Epoch 39/89\n",
            "10093/10093 [==============================] - 22s 2ms/step - loss: 4.3727 - mae: 0.9089 - mse: 4.3727\n",
            "Epoch 40/89\n",
            "10093/10093 [==============================] - 22s 2ms/step - loss: 4.3728 - mae: 0.9087 - mse: 4.3728\n",
            "Epoch 41/89\n",
            "10093/10093 [==============================] - 22s 2ms/step - loss: 4.3708 - mae: 0.9089 - mse: 4.3708\n",
            "Epoch 42/89\n",
            "10093/10093 [==============================] - 22s 2ms/step - loss: 4.3685 - mae: 0.9078 - mse: 4.3685\n",
            "Epoch 43/89\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.3693 - mae: 0.9078 - mse: 4.3693\n",
            "Epoch 44/89\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.3712 - mae: 0.9078 - mse: 4.3712\n",
            "Epoch 45/89\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.3711 - mae: 0.9076 - mse: 4.3711\n",
            "Epoch 46/89\n",
            "10093/10093 [==============================] - 22s 2ms/step - loss: 4.3710 - mae: 0.9076 - mse: 4.3710\n",
            "Epoch 47/89\n",
            "10093/10093 [==============================] - 22s 2ms/step - loss: 4.3715 - mae: 0.9078 - mse: 4.3715\n",
            "Epoch 48/89\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.3723 - mae: 0.9089 - mse: 4.3723\n",
            "Epoch 49/89\n",
            "10093/10093 [==============================] - 22s 2ms/step - loss: 4.3739 - mae: 0.9091 - mse: 4.3739\n",
            "Epoch 50/89\n",
            "10093/10093 [==============================] - 23s 2ms/step - loss: 4.3734 - mae: 0.9087 - mse: 4.3734\n",
            "Epoch 51/89\n",
            "10093/10093 [==============================] - 22s 2ms/step - loss: 4.3713 - mae: 0.9081 - mse: 4.3713\n",
            "Epoch 52/89\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.3704 - mae: 0.9080 - mse: 4.3704\n",
            "Epoch 53/89\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.3717 - mae: 0.9079 - mse: 4.3717\n",
            "Epoch 54/89\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.3716 - mae: 0.9084 - mse: 4.3716\n",
            "Epoch 55/89\n",
            "10093/10093 [==============================] - 22s 2ms/step - loss: 4.3677 - mae: 0.9077 - mse: 4.3677\n",
            "Epoch 56/89\n",
            "10093/10093 [==============================] - 22s 2ms/step - loss: 4.3715 - mae: 0.9084 - mse: 4.3715\n",
            "Epoch 57/89\n",
            "10093/10093 [==============================] - 22s 2ms/step - loss: 4.3693 - mae: 0.9081 - mse: 4.3693\n",
            "Epoch 58/89\n",
            "10093/10093 [==============================] - 22s 2ms/step - loss: 4.3701 - mae: 0.9080 - mse: 4.3701\n",
            "Epoch 59/89\n",
            "10093/10093 [==============================] - 22s 2ms/step - loss: 4.3686 - mae: 0.9076 - mse: 4.3686\n",
            "Epoch 60/89\n",
            "10093/10093 [==============================] - 22s 2ms/step - loss: 4.3679 - mae: 0.9084 - mse: 4.3679\n",
            "Epoch 61/89\n",
            "10093/10093 [==============================] - 22s 2ms/step - loss: 4.3696 - mae: 0.9079 - mse: 4.3696\n",
            "Epoch 62/89\n",
            "10093/10093 [==============================] - 22s 2ms/step - loss: 4.3679 - mae: 0.9078 - mse: 4.3679\n",
            "Epoch 63/89\n",
            "10093/10093 [==============================] - 22s 2ms/step - loss: 4.3695 - mae: 0.9077 - mse: 4.3695\n",
            "Epoch 64/89\n",
            "10093/10093 [==============================] - 23s 2ms/step - loss: 4.3714 - mae: 0.9078 - mse: 4.3714\n",
            "Epoch 65/89\n",
            "10093/10093 [==============================] - 23s 2ms/step - loss: 4.3695 - mae: 0.9082 - mse: 4.3695\n",
            "Epoch 66/89\n",
            "10093/10093 [==============================] - 23s 2ms/step - loss: 4.3700 - mae: 0.9080 - mse: 4.3700\n",
            "Epoch 67/89\n",
            "10093/10093 [==============================] - 23s 2ms/step - loss: 4.3693 - mae: 0.9080 - mse: 4.3693\n",
            "Epoch 68/89\n",
            "10093/10093 [==============================] - 23s 2ms/step - loss: 4.3691 - mae: 0.9085 - mse: 4.3691\n",
            "Epoch 69/89\n",
            "10093/10093 [==============================] - 22s 2ms/step - loss: 4.3715 - mae: 0.9091 - mse: 4.3715\n",
            "Epoch 70/89\n",
            "10093/10093 [==============================] - 22s 2ms/step - loss: 4.3709 - mae: 0.9091 - mse: 4.3709\n",
            "Epoch 71/89\n",
            "10093/10093 [==============================] - 23s 2ms/step - loss: 4.3697 - mae: 0.9090 - mse: 4.3697\n",
            "Epoch 72/89\n",
            "10093/10093 [==============================] - 22s 2ms/step - loss: 4.3707 - mae: 0.9092 - mse: 4.3707\n",
            "Epoch 73/89\n",
            "10093/10093 [==============================] - 22s 2ms/step - loss: 4.3717 - mae: 0.9089 - mse: 4.3717\n",
            "Epoch 74/89\n",
            "10093/10093 [==============================] - 21s 2ms/step - loss: 4.3752 - mae: 0.9098 - mse: 4.3752\n",
            "Epoch 75/89\n",
            "10093/10093 [==============================] - 22s 2ms/step - loss: 4.3723 - mae: 0.9094 - mse: 4.3723\n",
            "Epoch 76/89\n",
            "10093/10093 [==============================] - 22s 2ms/step - loss: 4.3692 - mae: 0.9087 - mse: 4.3692\n",
            "Epoch 77/89\n",
            "10093/10093 [==============================] - 23s 2ms/step - loss: 4.3687 - mae: 0.9086 - mse: 4.3687\n",
            "Epoch 78/89\n",
            "10093/10093 [==============================] - 23s 2ms/step - loss: 4.3704 - mae: 0.9084 - mse: 4.3704\n",
            "Epoch 79/89\n",
            "10093/10093 [==============================] - 23s 2ms/step - loss: 4.3697 - mae: 0.9089 - mse: 4.3697\n",
            "Epoch 80/89\n",
            "10093/10093 [==============================] - 23s 2ms/step - loss: 4.3672 - mae: 0.9084 - mse: 4.3672\n",
            "Epoch 81/89\n",
            "10093/10093 [==============================] - 23s 2ms/step - loss: 4.3690 - mae: 0.9087 - mse: 4.3690\n",
            "Epoch 82/89\n",
            "10093/10093 [==============================] - 22s 2ms/step - loss: 4.3675 - mae: 0.9084 - mse: 4.3675\n",
            "Epoch 83/89\n",
            "10093/10093 [==============================] - 22s 2ms/step - loss: 4.3714 - mae: 0.9088 - mse: 4.3714\n",
            "Epoch 84/89\n",
            "10093/10093 [==============================] - 22s 2ms/step - loss: 4.3682 - mae: 0.9085 - mse: 4.3682\n",
            "Epoch 85/89\n",
            "10093/10093 [==============================] - 22s 2ms/step - loss: 4.3686 - mae: 0.9083 - mse: 4.3686\n",
            "Epoch 86/89\n",
            "10093/10093 [==============================] - 22s 2ms/step - loss: 4.3678 - mae: 0.9090 - mse: 4.3678\n",
            "Epoch 87/89\n",
            "10093/10093 [==============================] - 22s 2ms/step - loss: 4.3701 - mae: 0.9086 - mse: 4.3701\n",
            "Epoch 88/89\n",
            "10093/10093 [==============================] - 22s 2ms/step - loss: 4.3675 - mae: 0.9085 - mse: 4.3675\n",
            "Epoch 89/89\n",
            "10093/10093 [==============================] - 22s 2ms/step - loss: 4.3691 - mae: 0.9084 - mse: 4.3691\n",
            "16221/16221 [==============================] - 24s 1ms/step - loss: 4.4872 - mae: 0.8954 - mse: 4.4872\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4.487170696258545, 0.8953806161880493, 4.487170696258545]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6lEC-fKky4O"
      },
      "source": [
        "##8. Resultado final do tunning\n",
        "O melhor modelo após o tunning foi o de 2 camadas densas com 30 neurinios cada e taxa de aprendizagem de 0.001 \n"
      ]
    }
  ]
}